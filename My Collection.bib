@article{Han2012,
abstract = {Elastic resource provisioning is a key feature of cloud computing, allowing users to scale up or down resource allocation for their applications at run-time. To date, most practical approaches to managing elasticity are based on allocation/de-allocation of the virtual machine (VM) instances to the application. This VM-level elasticity typically incurs both considerable overhead and extra costs, especially for applications with rapidly fluctuating demands. In this paper, we propose a lightweight approach to enable cost-effective elasticity for cloud applications. Our approach operates fine-grained scaling at the resource level itself (CPUs, memory, I/O, etc) in addition to VM-level scaling. We also present the design and implementation of an intelligent platform for light-weight resource management of cloud applications. We describe our algorithms for light-weight scaling and VM-level scaling and show their interaction. We then use an industry standard benchmark to evaluate the effectiveness of our approach and compare its performance against traditional approaches. {\textcopyright} 2012 IEEE.},
author = {Han, Rui and Guo, Li and Ghanem, Moustafa M. and Guo, Yike},
doi = {10.1109/CCGrid.2012.52},
file = {:E\:/文章/控制理论Lightweight Resource Scaling for Cloud Applications.pdf:pdf},
isbn = {9780769546919},
journal = {Proceedings - 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGrid 2012},
keywords = {cloud computing,lightweight scaling,resource allocation algorithms},
pages = {644--651},
title = {{Lightweight resource scaling for cloud applications}},
year = {2012}
}
@article{Lu2006,
abstract = {This paper presents the design and implementation of an adaptive Web server architecture to provide relative and absolute connection delay guarantees for different service classes. The first contribution of this paper is an adaptive architecture based on feedback control loops that enforce desired connection delays via dynamic connection scheduling and process reallocation. The second contribution is the use of control theoretic techniques to model and design the feedback loops with desired dynamic performance. In contrast to heuristics-based approaches that rely on laborious hand-tuning and testing iteration, the control theoretic approach enables systematic design of an adaptive Web server with established analytical methods. The adaptive architecture has been implemented by modifying an Apache server. Experimental results demonstrate that the adaptive server provides robust delay guarantees even when workload varies significantly. {\textcopyright} 2006 IEEE.},
author = {Lu, Chenyang and Lu, Ying and Abdelzaher, Tarek F. and Stankovic, John A. and Son, Sang Hyuk},
doi = {10.1109/TPDS.2006.123},
file = {:E\:/文章/控制理论/控制理论Feedback Control Architecture and Design.pdf:pdf},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Feedback control,Proportional differentiated service,Quality of Service,Web server},
number = {9},
pages = {1014--1027},
title = {{Feedback control architecture and design methodology for service delay guarantees in web servers}},
volume = {17},
year = {2006}
}
@article{Qu2016,
abstract = {Cloud providers sell their idle capacity on markets through an auction-like mechanism to increase their return on investment. The instances sold in this way are called spot instances. In spite that spot instances are usually 90% cheaper than on-demand instances, they can be terminated by provider when their bidding prices are lower than market prices. Thus, they are largely used to provision fault-tolerant applications only. In this paper, we explore how to utilize spot instances to provision web applications, which are usually considered as availability-critical. The idea is to take advantage of differences in price among various types of spot instances to reach both high availability and significant cost saving. We first propose a fault-tolerant model for web applications provisioned by spot instances. Based on that, we devise novel cost-efficient auto-scaling polices that comply with the defined fault-tolerant semantics for hourly billed cloud markets. We implemented the proposed model and policies both on a simulation testbed for repeatable validation and Amazon EC2. The experiments on the simulation testbed and EC2 show that the proposed approach can greatly reduce resource cost and still achieve satisfactory Quality of Service (QoS) in terms of response time and availability.},
archivePrefix = {arXiv},
arxivId = {1509.05197},
author = {Qu, Chenhao and Calheiros, Rodrigo N. and Buyya, Rajkumar},
doi = {10.1016/j.jnca.2016.03.001},
eprint = {1509.05197},
file = {:E\:/文章/A-reliable-and-cost-efficient-auto-.pdf:pdf},
issn = {10958592},
journal = {Journal of Network and Computer Applications},
keywords = {Auto-scaling,Cloud computing,Cost,Fault tolerant,QoS,Spot instance,Web application},
pages = {167--180},
publisher = {Elsevier},
title = {{A reliable and cost-efficient auto-scaling system for web applications using heterogeneous spot instances}},
url = {http://dx.doi.org/10.1016/j.jnca.2016.03.001},
volume = {65},
year = {2016}
}
@article{Padala2009,
abstract = {Virtualized data centers enable sharing of resources among hosted applications. However, it is difficult to satisfy service-level objectives (SLOs) of applications on shared infrastructure, as application workloads and resource consumption patterns change over time. In this paper, we present AutoControl, a resource control system that automatically adapts to dynamic workload changes to achieve application SLOs. AutoControl is a combination of an online model estimator and a novel multi-input, multi-output (MIMO) resource controller. The model estimator captures the complex relationship between application performance and resource allocations, while the MIMO controller allocates the right amount of multiple virtualized resources to achieve application SLOs. Our experimental evaluation with RUBiS and TPC-W benchmarks along with production-trace-driven workloads indicates that AutoControl can detect and mitigate CPU and disk I/O bottlenecks that occur over time and across multiple nodes by allocating each resource accordingly. We also show that AutoControl can be used to provide service differentiation according to the application priorities during resource contention. Copyright {\textcopyright} 2009 ACM.},
author = {Padala, Pradeep and Hou, Kai Yuan and Shin, Kang G. and Zhu, Xiaoyun and Uysal, Mustafa and Wang, Zhikui and Singhal, Sharad and Merchant, Arif},
doi = {10.1145/1519065.1519068},
file = {:E\:/文章/控制理论自适应控制Automated control of multiple virtualized resources.pdf:pdf},
isbn = {9781605584829},
journal = {Proceedings of the 4th ACM European Conference on Computer Systems, EuroSys'09},
keywords = {Application QoS,Automated control,Control theory,Data center,Resource management,Server consolidation,Virtualization},
pages = {13--26},
title = {{Automated control of multiple virtualized resources}},
year = {2009}
}
@article{Waltz1965,
abstract = {This paper describes a learning control system using a reinforcement technique. The controller is capable of controlling a plant that may be nonlinear and nonstationary. The only a priori information required by the controller is the order of the plant. The approach is to design a controller which partitions the control measurement space into sets called controI situations and then learns the best control choice for each control situation. The control measurements are those indicating the state of the plant and environment. The learning is accomplished by reinforcement of the probability of choosing a particular control choice for a given control situation. The system was stimulated on an IJ3M 1710-GEDA hybrid computer facility. Experimental results obtained from the simulation are presented .},
author = {Waltz, M and Fu, K},
file = {:E\:/文章/控制系统和强化学习A heuristic approach to reinforcement learning control systems.pdf:pdf},
journal = {IEEE Transactions on Automatic Control},
number = {4},
pages = {390--398},
title = {{IEEE TRANSACTIONS ON AUTOMATIC CONTROL A Heuristic Approach to Reinforcement Learning Control Systems}},
volume = {10},
year = {1965}
}
@article{Sha2002,
abstract = {Controlling the timing performance of a network server is a challenging problem. This paper presents a Queueing Model Based Feedback Control approach to keep the timing performance of a network server close to the service level specification. We show that in an instrumented Apache server, combining feedback control with a queueing model leads to better tracking of QoS specifications than with feedback control alone or queueing model based feed forward control alone.},
author = {Sha, Lui and Liu, Xue and Lu, Ying and Abdelzaher, Tarek},
doi = {10.1109/REAL.2002.1181564},
file = {:E\:/文章/Queueing model based network server performance control.pdf.pdf:pdf},
issn = {10528725},
journal = {Proceedings-Real-Time Systems Symposium},
pages = {81--90},
publisher = {IEEE},
title = {{Queueing model based network server performance control}},
year = {2002}
}
@article{Cai2021,
abstract = {Container orchestration platforms such as Kubernetes and Kubernetes-derived KubeEdge (called Kubernetes-based systems collectively) have been gradually used to conduct unified management of Cloud, Fog and Edge resources. Container provisioning algorithms are crucial to guaranteeing quality of services (QoS) of such Kubernetes-based systems. However, most existing algorithms focus on placement and migration of fixed number of containers without considering elastic provisioning of containers. Meanwhile, widely used linear-performance-model-based feedback control or fixed-processing-rate based queuing model on diverse platforms cannot describe the performance of containerized Web systems accurately. Furthermore, a fixed reference point used by existing methods is likely to generate inaccurate output errors incurring great fluctuations encountered with large arrival-rate changes. In this paper, a feedback control method is designed based on a combination of varying-processing-rate queuing model and linear-model to provision containers elastically which improves the accuracy of output errors by learning reference models for different arrival rates automatically and mapping output errors from reference models to the queuing model. Our approach is compared with several state-of-art algorithms on a real Kubernetes cluster. Experimental results illustrate that our approach obtains the lowest percentage of service level agreement (SLA) violation (decreasing no less than 8.44%) and the second lowest cost.},
author = {Cai, Zhicheng and Buyya, Rajkumar},
doi = {10.1109/TC.2021.3049598},
file = {:E\:/文章/Inverse Queuing Model based Feedback Control.pdf:pdf},
issn = {15579956},
journal = {IEEE Transactions on Computers},
keywords = {Adaptation models,Cloud,Cloud computing,Computational modeling,Container auto-scaling,Containers,Feedback control,Fog and Edge Computing,Kubernetes,Qos control,Quality of service,Queuing theory,Time factors},
pages = {1},
title = {{Inverse Queuing Model based Feedback Control for Elastic Container Provisioning of Web Systems in Kubernetes}},
year = {2021}
}
@article{Patikirikorala2011,
abstract = {Many control theory based approaches have been proposed to provide QoS assurance in increasingly complex software systems. These approaches generally use single model based, fixed or adaptive control techniques for QoS management of such systems. With varying system dynamics and unpredictable environmental changes, however, it is difficult to design a single model or controller to achieve the desired QoS performance across all the operating regions of these systems. In this paper, we propose a multi-model framework to capture the multi-model nature of software systems and implement self-managing control systems for them. A reference-model and extendable class library are introduced to implement such self-managing control systems. The proposed approach is also validated and compared to fixed and adaptive control schemes through a range of experiments. {\textcopyright} 2011 ACM.},
author = {Patikirikorala, Tharindu and Colman, Alan and Han, Jun and Wang, Liuping},
doi = {10.1145/1988008.1988040},
file = {:E\:/文章/控制理论A multi-model framework to implement self-managing control systems for QoS management.pdf:pdf},
isbn = {9781450305754},
issn = {02705257},
journal = {Proceedings - International Conference on Software Engineering},
keywords = {Adaptive control,Feedback control,Multi-model,Quality of service,Reconfiguring control,Self-managing systems},
pages = {218--227},
title = {{A multi-model framework to implement self-managing control systems for QoS management}},
year = {2011}
}
@article{Dutreilh2010,
abstract = {Continuously adjusting the horizontal scaling of applications hosted by data centers appears as a good candidate to automatic control approaches allocating resources in closed-loop given their current workload. Despite several attempts, real applications of these techniques in cloud computing infrastructures face some difficulties. Some of them essentially turn back to the core concepts of automatic control: controllability, inertia of the controlled system, gain and stability. In this paper, considering our recent work to build a management framework dedicated to automatic resource allocation in virtualized applications, we attempt to identify from experiments the sources of instabilities in the controlled systems. As examples, we analyze two types of policies: threshold-based and reinforcement learning techniques to dynamically scale resources. The experiments show that both approaches are tricky and that trying to implement a controller without looking at the way the controlled system reacts to actions, both in time and in amplitude, is doomed to fail. We discuss both lessons learned from the experiments in terms of simple yet key points to build good resource management policies, and longer term issues on which we are currently working to manage contracts and reinforcement learning efficiently in cloud controllers. {\textcopyright} 2010 IEEE.},
author = {Dutreilh, Xavier and Rivierre, Nicolas and Moreau, Aur{\'{e}}lien and Malenfant, Jacques and Truck, Isis},
doi = {10.1109/CLOUD.2010.55},
file = {:E\:/文章/控制理论From Data Center Resource Allocation to Control Theory and Back.pdf:pdf},
isbn = {9780769541303},
journal = {Proceedings - 2010 IEEE 3rd International Conference on Cloud Computing, CLOUD 2010},
keywords = {Application hosting,Closed loop systems,Cloud computing,Controllability,Hysteresis,Resource allocation},
pages = {410--417},
title = {{From data center resource allocation to control theory and back}},
year = {2010}
}
@inproceedings{Cai2020,
abstract = {Most existing quality of service (QoS) control algorithms of Web applications take into account Web Server or database connections which can be released immediately. However, many applications are deployed on virtual machines (VMs) or even Spot VMs elastically rented from public Clouds. To save costs, interval-priced VMs are not released until the ends of rented intervals. Such delays of control effects make existing methods rent or release excess VMs leading to overcontrol. Fluctuated prices make Spot VMs unreliable due to unexpected termination which makes fault-tolerant strategies crucial. In this article, an unequal-interval-based loosely coupled control method is proposed to improve the quality of service (QoS) control ability of fault-tolerant strategies. A queuing model with arrival-rate-adjustment coefficient is used to predict required capacity as a feedforward controller. Another two-threshold and queuing-model-based method is applied to update the coefficient as a loosely coupled feedback controller. Meanwhile, unequal-interval controller collaborating method is proposed to avoid overcontrol and react quickly to workload changes. Our approach is evaluated on both a simulation platform and a real Kubernetes Cluster. Experimental results illustrate that our approach decreases the percentage of waiting times larger than service level agreements with similar or lower rental costs compared with existing algorithms.},
author = {Cai, Zhicheng and Liu, Duan and Lu, Yifei and Buyya, Rajkumar},
booktitle = {Concurrency Computation},
doi = {10.1002/cpe.5926},
issn = {15320634},
keywords = {Cloud computing,Spot VM,feedback control,queuing model,resource provisioning},
number = {23},
pages = {e5926},
title = {{Unequal-interval based loosely coupled control method for auto-scaling heterogeneous cloud resources for web applications}},
volume = {32},
year = {2020}
}
@article{Baresi2016,
abstract = {Modern Web applications exploit Cloud infrastructures to scale their resources and cope with sudden changes in the workload. While the state of practice is to focus on dynamically adding and removing virtual machines, we advocate that there are strong benefits in containerizing the applications and in scaling the containers. In this paper we present an autoscaling technique that allows containerized applications to scale their resources both at the virtual machine (VM) level and at the container level. Furthermore, applications can combine this infrastructural adaptation with platform-level adaptation. The autoscaling is made possible by our planner, which consists of a grey-box discrete-Time feedback controller. The work has been validated using two application benchmarks deployed to Amazon EC2. Our experiments show that our planner outperforms Amazon's AutoScaling by 78% on average without containers; and that the introduction of containers allows us to improve by yet another 46% on average.},
author = {Baresi, Luciano and Guinea, Sam and Leva, Alberto and Quattrocchi, Giovanni},
doi = {10.1145/2950290.2950328},
file = {:E\:/文章/控制理论/A discrete-time feedback controller for containerized cloud applications.pdf:pdf;:E\:/文章/控制理论A discrete time feedback controller for containerized cloud applications.pdf:pdf},
isbn = {9781450342186},
journal = {Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
keywords = {,Adaptive Systems,Cloud Computing,Containers,Control Theory,Software Adaptation},
pages = {217--228},
title = {{A discrete-Time feedback controller for containerized cloud applications}},
volume = {13-18-Nove},
year = {2016}
}
@article{Wenping2008,
abstract = {This paper considers providing two types of QoS guarantees, proportional delay differentiation (PDD) and absolute delay guarantee (ADG), in the database connection pool (DBCP) for web application servers using the classical feedback control theory. PDD aims to maintain the average queuing delay ratio between different classes of requests according to pre-specified parameters, and ADG aims to ensure the average queueing delay for requests with high priority is no more than the threshold configured. To achieve these goals, We establish the approximate linear time-invariant models of the DBCP through system identification experimentally, and design two proportional-integral (PI) controllers, PDD controller and ADG controller, using the Root Locus method. These controllers are invoked periodically to calculate and adjust the probabilities for different classes of requests to use a limited number of database connections, according to the error between the measured QoS metric and the reference value. We implement all components of the closed-loops in a real DBCP for web application servers, and design three kinds of workloads, which follow deterministic, uniform and heavy-tailed distributions respectively, to evaluate the performance of the closed-loop systems. Experiment results demonstrate that, the controllers designed are effective in handling varying workloads, PDD and ADG can be achieved in the web application server even if the number of concurrent requests changes abruptly. {\textcopyright} 2008 IEEE.},
author = {Wenping, Pan and Dejun, Mu and Hangxing, Wu and Lei, Yao},
doi = {10.1109/HPCC.2008.106},
file = {:E\:/文章/控制理论Feedback Control-Based QoS Guarantees in Web Application Servers.pdf:pdf},
isbn = {9780769533520},
journal = {Proceedings - 10th IEEE International Conference on High Performance Computing and Communications, HPCC 2008},
pages = {328--334},
title = {{Feedback control-based QoS guarantees in web application servers}},
year = {2008}
}
@article{Huang2014,
abstract = {Web applications are mostly designed with multiple tiers for flexibility and software reusability. It is difficult to model the behavior of multi-tier Web applications due to the fact that the workload is dynamic and unpredictable and the resource demand in each tier is different. Those features also cause the task of resource allocation for multi-tier Web applications very challenging. In order to meet service level agreements (SLAs) with minimal resource costs, Web service providers should dynamically allocate appropriate resources to each tier. This is particularly important to minimize the monetary cost in the pay-as-you-go cloud computing environments. Recently, a number of rule and model based approaches have been proposed for resource provisioning in cloud computing. In this survey, we identify challenges of the resource allocation problem and conduct a comparative review on those rule and model based approaches for resource allocation in multi-tier Web sites. Given the analysis on their advantages and limitations, we outline research directions to further improve the effectiveness of resource management in multi-tier Web applications. {\textcopyright} 2014 IEEE.},
author = {Huang, Dong and He, Bingsheng and Miao, Chunyan},
doi = {10.1109/SURV.2014.010814.00060},
file = {:E\:/文章/A Survey of Resource Management in Multi-Tier.pdf:pdf;:E\:/文章/A survey of resource management in multi-tier web applications.pdf:pdf},
issn = {1553877X},
journal = {IEEE Communications Surveys and Tutorials},
keywords = {,Web service,control theory,learning,multi-tier architecture,resource allocation},
number = {3},
pages = {1574--1590},
publisher = {IEEE},
title = {{A survey of resource management in multi-tier web applications}},
volume = {16},
year = {2014}
}
@article{JafarnejadGhomi2019,
abstract = {The cloud computing paradigm is an important service in the Internet for sharing and providing resources in a cost-efficient way. Modeling of a cloud system is not an easy task because of the complexity and large scale of such systems. Cloud reliability could be improved by modeling the various aspects of cloud systems, including scheduling, service time, wait time, and hardware and software failures. The aim of this study is to survey research studies done on the modeling of cloud computing using the queuing system in order to identify where more emphasis should be placed in both current and future research directions. This paper follows the goal by investigating the articles published between 2008 and January 2017 in journals and conferences. A systematic mapping study combined with a systematic literature review was performed to find the related literature, and 71 articles were selected as primary studies that were classified in relation to the focus, research type, and contribution type. We classified the modeling techniques of cloud computing using the queuing theory in seven categories based on their focus area: (1) performance, (2) quality of service, (3) workflow scheduling, (4) energy savings, (5) resource management, (6) priority-based servicing, and (7) reliability. A majority of the primary articles focus on performance (37%), 15% of them focus on resource management, 14% of them focus on quality of service, 13% of them focus on workflow scheduling, 13% of them focus on energy savings, 4% of them focus on priority-based servicing for requests, and 4% of them focus on reliability. This work summarizes and classifies the research efforts conducted on applying queue theory for modeling of cloud computing (AQTMCC), providing a good starting point for further research in this area.},
author = {{Jafarnejad Ghomi}, Einollah and Rahmani, Amir Masoud and Qader, Nooruldeen Nasih},
doi = {10.1002/cpe.5186},
file = {:E\:/文章/Applying queue theory for modeling of cloud computing：A systematic review.pdf:pdf;:E\:/文章/Applying queue theory for modeling of cloud compu.pdf:pdf},
issn = {15320634},
journal = {Concurrency Computation },
keywords = {,cloud computing,cloud modeling,queuing systems,queuing theory,systematic review},
number = {17},
pages = {1--31},
title = {{Applying queue theory for modeling of cloud computing: A systematic review}},
volume = {31},
year = {2019}
}
@article{Naik2016,
abstract = {Virtualization is the nucleus of the cloud computing for providing its services on-demand. Cloud-based distributed systems are predominantly developed using virtualization technology. However, the requirement of significant resources and issues of interoperability and deployment make it less adoptable in the development of many types of distributed systems. Dockerization or Docker Container-based virtualization has been introduced in the last three years and gaining popularity in the software development community. Docker has recently introduced its distributed system development tool called Swarm, which extends the Docker Container-based system development process on multiple hosts in multiple clouds. Docker Swarm-based containerized distributed system is a brand new approach and needs to be compared with the virtualized distributed system. Therefore, this paper presents the simulation and evaluation of the development of a distributed system using virtualization and dockerization. This simulation is based on Docker Swarm, VirtualBox, Ubuntu, Mac OS X, nginx and redis. To simulate and evaluate the distributed system in the same environment, all Swarm Nodes and Virtual Machines are created using VirtualBox on the same Mac OS X host. For making this evaluation rational, almost similar system resources are allocated to both at the beginning. Subsequently, similar servers nginx and redis are installed on the Swarm Node and Virtual Machine. Finally, based on the experimental simulation results, it evaluates their required resources and operational overheads; thus, their performance and effectiveness for designing distributed systems.},
author = {Naik, Nitin},
doi = {10.1109/MESOCA.2016.9},
file = {:E\:/文章/Migrating from Virtualization to Dockerization in.pdf:pdf;:E\:/文章/Migrating from Virtualization to Dockerization in the Cloud Simulation and Evaluation of Distributed Systems.pdf:pdf},
isbn = {9781509038527},
journal = {Proceedings - 2016 IEEE 10th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Environments, MESOCA 2016},
keywords = {,Cloud,Container,Distributed Systems,Docker,Dockerization,Hypervisor,Virtual Machine,Virtual Machine Monitor,Virtualization},
pages = {1--8},
publisher = {IEEE},
title = {{Migrating from Virtualization to Dockerization in the Cloud: Simulation and Evaluation of Distributed Systems}},
year = {2016}
}
@article{Li2011,
abstract = {In this paper, we propose and implement a control mechanism that interfaces with Infrastructure as a Service (IaaS) or "cloud" providers to provision resources and manage instances of web applications in response to volatile and complex request patterns. We use reinforcement learning to orchestrate control actions such as provisioning servers and application placement to meet performance requirements and minimize ongoing costs. The mechanism is incorporated in a distributed, elastic hosting architecture that is evaluated using actual web applications running on resources from Amazon EC2. {\textcopyright} 2011 ACM.},
author = {Li, Han and Venugopal, Srikumar},
doi = {10.1145/1998582.1998630},
file = {:E\:/文章/Using reinforcement learning for controlling an elastic web application hosting platform.pdf:pdf},
isbn = {9781450306072},
journal = {Proceedings of the 8th ACM International Conference on Autonomic Computing, ICAC 2011 and Co-located Workshops},
keywords = {,elastic computing,provisioning,reinforcement learning},
pages = {205--208},
title = {{Using reinforcement learning for controlling an elastic web application hosting platform}},
year = {2011}
}
@inproceedings{Melikov2018,
abstract = {The objective of the paper is to analyzed QoS metric of cloud computing with large-scale of web server from queue management perspective. We propose a new method in order to effectively calculate the steady-state probabilities of cloud system with a large number of web servers. Numerical results showed that the proposed algorithms have higher accuracy and negligible computation time. Taking into account that in the cloud computing repair of server is taking some hours and response time is handling within some seconds, we can correctly apply space-merging algorithm.},
address = {Cham},
author = {Melikov, A. Z. and Rustamov, A. M. and Sztrik, J.},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-319-99447-5_10},
editor = {{Vishnevskiy Vladimir M.} and and Kozyrev, Dmitry V},
isbn = {9783319994468},
issn = {18650929},
keywords = {Cloud computing,Cloud technology,Queuing system},
pages = {106--119},
publisher = {Springer International Publishing},
title = {{Queuing management with feedback in cloud computing centers with large numbers of web servers}},
volume = {919},
year = {2018}
}
@inproceedings{Lei2020,
abstract = {Most of existing resource provisioning methods are designed for traditional Web applications with linear structures. However, Web systems with the meshed topology are becoming widespread. Meshed connections among different tiers make Virtual Machine (VM) provisioning and bottleneck elimination complex. In this paper, a Jackson network based Proactive and Reactive VM auto-scaling Method (JPRM) is proposed. In JPRM, request transition behaviors among tiers are modeled as a finite-state Markov stochastic process. A transition probability matrix is studied on-line to predict resource requirements based on M/M/N queuing models as proactive control. For reactive provisioning, the final increased request rate of each tier is determined based on stable state checking and Jackson equilibrium equation solving to eliminate bottleneck tiers and avoid bottleneck shifting. The JPRM is evaluated in a simulation environment established using CloudSim. Experimental results show that JPRM avoids bottleneck shifting with reasonable additional VM rental costs compared with existing methods.},
author = {Lei, Yamin and Cai, Zhicheng and Wu, Hang and Buyya, Rajkumar},
booktitle = {IEEE International Conference on Cloud Computing, CLOUD},
doi = {10.1109/CLOUD49709.2020.00076},
file = {:E\:/文章/论文.pdf:pdf},
isbn = {9781728187808},
issn = {21596190},
keywords = {Bottleneck eliminating,Cloud computing,Jackson network,Meshed Web systems,Resource provisioning},
pages = {512--516},
title = {{Cloud resource provisioning and bottleneck eliminating for meshed web systems}},
volume = {2020-Octob},
year = {2020}
}
@article{Sotiriadis2019,
abstract = {Today, cloud computing applications are rapidly constructed by services belonging to different cloud providers and service owners. This work presents the inter-cloud elasticity framework, which focuses on cloud load balancing based on dynamic virtual machine reconfiguration when variations on load or on user requests volume are observed. We design a dynamic reconfiguration system, called inter-cloud load balancer (ICLB), that allows scaling up or down the virtual resources (thus providing automatized elasticity), by eliminating service downtimes and communication failures. It includes an inter-cloud load balancer for distributing incoming user HTTP traffic across multiple instances of inter-cloud applications and services and we perform dynamic reconfiguration of resources according to the real time requirements. The experimental analysis includes different topologies by showing how real-time traffic variation (using real world workloads) affects resource utilization and by achieving better resource usage in inter-cloud.},
author = {Sotiriadis, Stelios and Bessis, Nik and Amza, Cristiana and Buyya, Rajkumar},
doi = {10.1109/TSC.2016.2634024},
file = {:E\:/文章/Elastic Load Balancing for Dynamic Virtual Machine Reconfiguration Based on Vertical and Horizontal Scaling.pdf:pdf},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Cloud computing,cloud elasticity,cloud load balancing,horizontal scalability,vertical scalability},
number = {2},
pages = {319--334},
title = {{Elastic load balancing for dynamic virtual machine reconfiguration based on vertical and horizontal scaling}},
volume = {12},
year = {2019}
}
@misc{,
title = {{Apache jmeter: Workload generator}},
url = {https://jmeter.apache.org/},
year = {2021}
}
@article{Iqbal2016,
abstract = {Dynamic resource provisioning for Web applications allows for low operational costs while meeting service-level objectives (SLOs). However, the complexity of multitier Web applications makes it difficult to automatically provision resources for each tier without human supervision. In this paper, we introduce unsupervised machine learning methods to dynamically provision multitier Web applications, while observing user-defined performance goals. The proposed technique operates in real time and uses learning techniques to identify workload patterns from access logs, reactively identifies bottlenecks for specific workload patterns, and dynamically builds resource allocation policies for each particular workload. We demonstrate the effectiveness of the proposed approach in several experiments using synthetic workloads on the Amazon Elastic Compute Cloud (EC2) and compare it with industry-standard rule-based autoscale strategies. Our results show that the proposed techniques would enable cloud infrastructure providers or application owners to build systems that automatically manage multitier Web applications, while meeting SLOs, without any prior knowledge of the applications' resource utilization or workload patterns.},
author = {Iqbal, Waheed and Dailey, Mathew N. and Carrera, David},
doi = {10.1109/JSYST.2015.2424998},
file = {:E\:/文章/Unsupervised Learning of Dynamic Resource Provisioning Policies for Cloud-Hosted Multitier Web Applications.pdf:pdf},
issn = {19379234},
journal = {IEEE Systems Journal},
keywords = {Cloud computing,multi-tier applications,resource management,scalability,service-level agreement (SLA),system performance},
number = {4},
pages = {1435--1446},
title = {{Unsupervised Learning of Dynamic Resource Provisioning Policies for Cloud-Hosted Multitier Web Applications}},
volume = {10},
year = {2016}
}
@inproceedings{Chen2017,
abstract = {Scaling web applications such as e-commerce in cloud by adding or removing servers in the system is an important practice to handle workload variations, with the goal of achieving both high quality of service (QoS) and high resource efficiency. Through extensive scaling experiments of an n-tier application benchmark (RUBBoS), we have observed that scaling only hardware resources without appropriate adaptation of soft resource allocations (e.g., thread or connection pool size) of each server would cause significant performance degradation of the overall system by either under-or over-utilizing the bottleneck resource in the system. We develop a dynamic concurrency management (DCM) framework which integrates soft resource allocations into the system scaling management. DCM introduces a model which determines a near-optimal concurrency setting to each tier of the system based on a combination of operational queuing laws and online analysis of fine-grained measurement data. We implement DCM as a two-level actuator which scales both hardware and soft resources in an n-tier system on the fly without interrupting the runtime system performance. Our experimental results demonstrate that DCM can achieve significantly more stable performance and higher resource efficiency compared to the state-of-the-art hardware-only scaling solutions (e.g., Amazon EC2-AutoScale) under realistic bursty workload traces.},
author = {Chen, Hui and Wang, Qingyang and Palanisamy, Balaji and Xiong, Pengcheng},
booktitle = {Proceedings - International Conference on Distributed Computing Systems},
doi = {10.1109/ICDCS.2017.22},
editor = {Lee, K and Liu, L},
file = {:E\:/文章/DCM Dynamic Concurrency Management for.pdf:pdf},
isbn = {9781538617915},
keywords = {Cloud scaling,Concurrency,N-tier system,Performance,Web application},
pages = {2097--2104},
title = {{DCM: Dynamic Concurrency Management for Scaling n-Tier Applications in Cloud}},
year = {2017}
}
@inproceedings{Wen2018,
abstract = {With the rapid development of internet technology, internet has brought great conveniences to the vast majority of users. However, the exploding of concurrent visits also raised huge challenges to the working capacity of network servers. Thus server cluster and load balancing technologies become a powerful approach to deal with the bottleneck of server terminals. The Nginx-based load balancing method, featured with its high cost performance and extensibility, has been widely applied. This paper raised a Nginx-based dynamic feedback load balancing algorithm. This loading algorithm module is realized by designing an algorithm on the basis of queuing theory and through Nginx plug-in mechanism. Then the paper established an experiment environment, and used Loadrunner software to conduct a comparison test on the performances of the Nginx built-in IP Hash algorithm Weighted Round-Robin Scheduling and the dynamic feedback algorithm raised in this paper. The results showed that dynamic algorithm raised in this paper can better realize the load balancing.},
author = {Wen, Zepeng and Li, Gongliang and Yang, Guanghong},
booktitle = {Proceedings of 2018 IEEE 3rd Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2018},
doi = {10.1109/IAEAC.2018.8577911},
file = {:E\:/文章/Research and Realization of Nginx-based Dynamic Feedback Load Balancing Algorithm.pdf:pdf},
isbn = {9781538645086},
keywords = {Cluster,Loadrunner,Nginx,load balancing,queuing theory},
number = {Iaeac},
pages = {2541--2546},
publisher = {IEEE},
title = {{Research and Realization of Nginx-based Dynamic Feedback Load Balancing Algorithm}},
year = {2018}
}
@misc{Alibaba2020,
author = {Alibaba},
title = {{Container Service: Application Lifecycle Management - Alibaba Cloud}},
url = {https://www.alibabacloud.com/product/kubernetes https://www.alibabacloud.com/product/container-service},
year = {2020}
}
@article{Qu2018,
abstract = {Web application providers have been migrating their applications to cloud data centers, attracted by the emerging cloud computing paradigm. One of the appealing features of the cloud is elasticity. It allows cloud users to acquire or release computing resources on demand, which enables web application providers to automatically scale the resources provisioned to their applications without human intervention under a dynamic workload to minimize resource cost while satisfying Quality of Service (QoS) requirements. In this article, we comprehensively analyze the challenges that remain in auto-scaling web applications in clouds and review the developments in this field. We present a taxonomy of auto-scalers according to the identified challenges and key properties. We analyze the surveyed works and map them to the taxonomy to identify the weaknesses in this field. Moreover, based on the analysis, we propose new future directions that can be explored in this area.},
archivePrefix = {arXiv},
arxivId = {1609.09224},
author = {Qu, Chenhao and Calheiros, Rodrigo N. and Buyya, Rajkumar},
doi = {10.1145/3148149},
eprint = {1609.09224},
file = {:E\:/文章/Auto-Scaling_Web_Applications_in_Clouds_A_Taxonomy.pdf:pdf},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Auto-scaling,Cloud computing,Web application},
number = {4},
title = {{Auto-scaling web applications in clouds: A taxonomy and survey}},
volume = {51},
year = {2018}
}
@inproceedings{Toka2020,
abstract = {Kubernetes, the prevalent container orchestrator for cloud-deployed web applications, offers an automatic scaling feature for the application provider in order to meet the ever-changing amount of demand from its clients. This auto-scaling service, however, requires a seemingly difficult parameter set to be customized by the application provider, and those management parameters are static while incoming web request dynamics often change, not to mention the fact that scaling decisions are inherently reactive, instead of being proactive. Therefore we set the ultimate goal of making cloud-based web applications' management easier and more effective.We propose a Kubernetes scaling engine that makes the auto-scaling decisions apt for handling the actual variability of incoming requests. In this engine various AI-based forecast methods compete with each other via a short-term evaluation loop in order to always give the lead to the method that suits best the actual request dynamics, as soon as possible. We also introduce a compact management parameter for the cloud-tenant application provider in order to easily set their sweet spot in the resource over-provisioning vs. SLA violation trade-off.The multi-forecast scaling engine and the proposed management parameter are evaluated both in simulations and with measurements on our collected web traces to show the improved quality of fitting provisioned resources to service demand. We find that with just a few competing forecast methods, our auto-scaling engine, implemented in Kubernetes, results in significantly less lost requests with slightly more provisioned resources compared to the default baseline.},
address = {Los Alamitos, CA, USA},
author = {Toka, Laszlo and Dobreff, Gergely and Fodor, Balazs and Sonkoly, Balazs},
booktitle = {Proceedings - 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, CCGRID 2020},
doi = {10.1109/CCGrid49817.2020.00-33},
file = {:E\:/文章/Adaptive AI-based auto-scaling for Kubernetes.pdf:pdf},
isbn = {9781728160955},
keywords = {Kubernetes,artificial intelligence,auto-scaling,cloud computing,forecast,resource management},
month = {may},
pages = {599--608},
publisher = {IEEE Computer Society},
title = {{Adaptive AI-based auto-scaling for Kubernetes}},
year = {2020}
}
@article{Li2018,
abstract = {As the most popular container technology, Docker can provide more efficient solution for virtualization. Swarm is a management tool for Docker cluster. Since it is simple and flexible, Swarm is suitable for building private cloud platform. However, the strategies Swarm currently supports cannot achieve good performance of load balancing in some cases. To solve this problem, we present a new scheduling strategy named Multi-Algorithm Collaboration Scheduling Strategy, which can satisfy various scheduling requirements by using different algorithm specifically designed for different cases to process requests in turns. The results of comparison experiment suggest that the presented strategy has a more comprehensive performance than other strategies for Swarm.},
author = {Li, Qilong and Fang, Yu},
doi = {10.1109/ICCSEC.2017.8446688},
file = {:E\:/文章/Multi-Algorithm Collaboration Scheduling Strategy for Docker Container.pdf:pdf},
isbn = {9781538635735},
journal = {2017 International Conference on Computer Systems, Electronics and Control, ICCSEC 2017},
keywords = {Component,Container,Docker,Load balance,Scheduler},
pages = {1367--1371},
publisher = {IEEE},
title = {{Multi-algorithm collaboration scheduling strategy for docker container}},
year = {2018}
}
@article{Qu2016a,
abstract = {Cloud providers sell their idle capacity on markets through an auction-like mechanism to increase their return on investment. The instances sold in this way are called spot instances. In spite that spot instances are usually 90% cheaper than on-demand instances, they can be terminated by provider when their bidding prices are lower than market prices. Thus, they are largely used to provision fault-tolerant applications only. In this paper, we explore how to utilize spot instances to provision web applications, which are usually considered as availability-critical. The idea is to take advantage of differences in price among various types of spot instances to reach both high availability and significant cost saving. We first propose a fault-tolerant model for web applications provisioned by spot instances. Based on that, we devise novel cost-efficient auto-scaling polices that comply with the defined fault-tolerant semantics for hourly billed cloud markets. We implemented the proposed model and policies both on a simulation testbed for repeatable validation and Amazon EC2. The experiments on the simulation testbed and EC2 show that the proposed approach can greatly reduce resource cost and still achieve satisfactory Quality of Service (QoS) in terms of response time and availability.},
archivePrefix = {arXiv},
arxivId = {1509.05197},
author = {Qu, Chenhao and Calheiros, Rodrigo N. and Buyya, Rajkumar},
doi = {10.1016/j.jnca.2016.03.001},
eprint = {1509.05197},
file = {:E\:/文章/A reliable and cost-efficient auto-scaling system for web applications using heterogeneous spot instances.pdf:pdf},
issn = {10958592},
journal = {Journal of Network and Computer Applications},
keywords = {Auto-scaling,Cloud computing,Cost,Fault tolerant,QoS,Spot instance,Web application},
pages = {167--180},
publisher = {Elsevier},
title = {{A reliable and cost-efficient auto-scaling system for web applications using heterogeneous spot instances}},
url = {http://dx.doi.org/10.1016/j.jnca.2016.03.001},
volume = {65},
year = {2016}
}
@inproceedings{Magableh2020,
abstract = {One desired aspect of a self-adapting microservices architecture is the ability to continuously monitor the operational environment, detect and observe anomalous behaviour as well as implement a reasonable policy for self-scaling, self-healing, and self-tuning the computational resources in order to dynamically respond to a sudden change in its operational environment. Often the behaviour of a microservices architecture continuously changes over time and the identification of both normal and abnormal behaviours of running services becomes a challenging task. This paper proposes a self-healing Microservice architecture that continuously monitors the operational environment, detects and observes anomalous behaviours, and provides a reasonable adaptation policy using a multi-dimensional utility-based model. This model preserves the cluster state and prevents multiple actions to taking place at the same time. It also guarantees that the executed adaptation action fits the current execution context and achieves the adaptation goals. The results show the ability of this model to dynamically scale the architecture horizontally or vertically in response to the context changes.},
address = {Cham},
author = {Magableh, Basel and Almiani, Muder},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-030-15032-7_71},
isbn = {9783030150310},
issn = {21945357},
keywords = {Anomaly detection,Microservices architecture,Run-time configuration,Self healing},
pages = {846--858},
publisher = {Springer International Publishing},
title = {{A Self Healing Microservices Architecture: A Case Study in Docker Swarm Cluster}},
volume = {926},
year = {2020}
}
@article{Urdaneta2009,
abstract = {We study an access trace containing a sample of Wikipedia's traffic over a 107-day period aiming to identify appropriate replication and distribution strategies in a fully decentralized hosting environment. We perform a global analysis of the whole trace, and a detailed analysis of the requests directed to the English edition of Wikipedia. In our study, we classify client requests and examine aspects such as the number of read and save operations, significant load variations and requests for nonexisting pages. We also review proposed decentralized wiki architectures and discuss how they would handle Wikipedia's workload. We conclude that decentralized architectures must focus on applying techniques to efficiently handle read operations while maintaining consistency and dealing with typical issues on decentralized systems such as churn, unbalanced loads and malicious participating nodes. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Urdaneta, Guido and Pierre, Guillaume and van Steen, Maarten},
doi = {10.1016/j.comnet.2009.02.019},
issn = {13891286},
journal = {Computer Networks},
keywords = {Decentralized hosting,P2P,Wikipedia,Workload analysis},
number = {11},
pages = {1830--1845},
title = {{Wikipedia workload analysis for decentralized hosting}},
volume = {53},
year = {2009}
}
@article{Imdoukh2020,
abstract = {Containers are shaping the new era of cloud applications due to their key benefits such as lightweight, very quick to launch, consuming minimum resources to run an application which reduces cost, and can be easily and rapidly scaled up/down as per workload requirements. However, container-based cloud applications require sophisticated auto-scaling methods that automatically and in a timely manner provision and de-provision cloud resources without human intervention in response to dynamic fluctuations in workload. To address this challenge, in this paper, we propose a proactive machine learning-based approach to perform auto-scaling of Docker containers in response to dynamic workload changes at runtime. The proposed auto-scaler architecture follows the commonly abstracted four steps: monitor, analyze, plan, and execute the control loop. The monitor component continuously collects different types of data (HTTP request statistics, CPU, and memory utilization) that are needed during the analysis and planning phase to determine proper scaling actions. We employ in analysis phase a concise yet fast, adaptive, and accurate prediction model based on long short-term memory (LSTM) neural network to predict future HTTP workload to determine the number of containers needed to handle requests ahead of time to eliminate delays caused by starting or stopping running containers. Moreover, in the planning phase, the proposed gradually decreasing strategy avoids oscillations which happens when scaling operations are too frequent. Experimental results using realistic workload show that the prediction accuracy of LSTM model is as accurate as auto-regression integrated moving average model but offers 600 times prediction speedup. Moreover, as compared with artificial neural network model, LSTM model performs better in terms of auto-scaler metrics related to provisioning and elastic speedup. In addition, it was observed that when LSTM model is used, the predicted workload helped in using the minimum number of replicas to handle future workload. In the experiments, the use of GDS showed promising results in keeping the desired performance at reduced cost to handle cases with sudden workload increase/decrease.},
author = {Imdoukh, Mahmoud and Ahmad, Imtiaz and Alfailakawi, Mohammad Gh},
doi = {10.1007/s00521-019-04507-z},
file = {:E\:/文章/Imdoukh2020_Article_MachineLearning-basedAuto-scal.pdf:pdf},
isbn = {0123456789},
issn = {14333058},
journal = {Neural Computing and Applications},
keywords = {Auto-scaling,Containerization,Long short-term memory,Neural network,Prediction,Proactive controller},
number = {13},
pages = {9745--9760},
publisher = {Springer London},
title = {{Machine learning-based auto-scaling for containerized applications}},
url = {https://doi.org/10.1007/s00521-019-04507-z},
volume = {32},
year = {2020}
}
@article{Toka2021,
abstract = {Kubernetes, the container orchestrator for cloud-deployed applications, offers automatic scaling for the application provider in order to meet the ever-changing intensity of processing demand. This auto-scaling feature can be customized with a parameter set, but those management parameters are static while incoming Web request dynamics often change, not to mention the fact that scaling decisions are inherently reactive, instead of being proactive. We set the ultimate goal of making cloud-based applications' management easier and more effective. We propose a Kubernetes scaling engine that makes the auto-scaling decisions apt for handling the actual variability of incoming requests. In this engine various machine learning forecast methods compete with each other via a short-term evaluation loop in order to always give the lead to the method that suits best the actual request dynamics. We also introduce a compact management parameter for the cloud-tenant application provider to easily set their sweet spot in the resource over-provisioning vs. SLA violation trade-off. We motivate our scaling solution with analytical modeling and evaluation of the current Kubernetes behavior. The multi-forecast scaling engine and the proposed management parameter are evaluated both in simulations and with measurements on our collected Web traces to show the improved quality of fitting provisioned resources to service demand. We find that with just a few, but fundamentally different, and competing forecast methods, our auto-scaler engine, implemented in Kubernetes, results in significantly fewer lost requests with just slightly more provisioned resources compared to the default baseline.},
author = {Toka, Laszlo and Dobreff, Gergely and Fodor, Bal{\'{a}}zs and Sonkoly, Bal{\'{a}}zs},
doi = {10.1109/TNSM.2021.3052837},
file = {:E\:/文章/Machine Learning-Based Scaling Management for Kubernetes.pdf:pdf},
issn = {19324537},
journal = {IEEE Transactions on Network and Service Management},
keywords = {Cloud computing,Kubernetes,auto-scaling,forecast,machine learning,resource management},
number = {1},
pages = {958--972},
title = {{Machine Learning-Based Scaling Management for Kubernetes Edge Clusters}},
volume = {18},
year = {2021}
}
@article{Xu2019,
abstract = {VM consolidation and Dynamic Voltage Frequency Scaling approaches have been proved to be efficient to reduce energy consumption in cloud data centers. However, the existing approaches cannot function efficiently when the whole data center is overloaded. An approach called brownout has been proposed to solve the limitation, which dynamically deactivates or activates optional microservices or containers. In this paper, we propose a brownout-based software system for container-based clouds to handle overloads and reduce power consumption. We present its design and implementation based on Docker Swarm containers. The proposed system is integrated with existing Docker Swarm without the modification of their configurations. To demonstrate the potential of BrownoutCon software in offering energy-efficient services in brownout situation, we implemented several policies to manage containers and conducted experiments on French Grid'5000 cloud infrastructure. The results show the currently implemented policies in our software system can save about 10%–40% energy than the existing baselines while ensuring quality of services.},
author = {Xu, Minxian and Buyya, Rajkumar},
doi = {10.1016/j.jss.2019.05.031},
file = {:E\:/文章/BrownoutCon.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Brownout,Cloud data centers,Containers,Energy efficiency,Microservices,Quality of service},
pages = {91--103},
publisher = {Elsevier Inc.},
title = {{BrownoutCon: A software system based on brownout and containers for energy-efficient cloud computing}},
url = {https://doi.org/10.1016/j.jss.2019.05.031},
volume = {155},
year = {2019}
}
@article{Nanda2016,
abstract = {In the competitive world of modern web applications, performance plays a crucial role. An e-commerce company estimated that every 100ms delay reduces sales by 1 percent, and a popular search engine reported that every 500ms delay in search reduces earnings by 20 percent. The demands from users for these services can vary widely based on factors such as the time-of-day and unexpected events that can trigger flash crowds. To meet these demands web applications can be organized using a multi-tier architecture to make them modular and scalable in a cloud environment. However, a highly dynamic workload and different types of resource requirements in each tier can make it difficult to model the behavior of these applications. This presents two significant challenges to infrastructure providers: 1) to model the behavior of an application workload and provide responsive resources using dynamic resource provisioning, and 2) to maintain performance-based (response time) Service Level Agreements (SLAs). In this paper, we formulate a convex optimization problem for resource allocation, and offer a strict SLA for performance. We adopt an SLA violation cost model to formulate our optimization problem and derive the solution for dynamic resource provisioning. To achieve a strict SLA for the response time of an application, we propose a predictive model that seeks to dynamically provision resources using a Feedback-based Control System (FCS). Our model is applicable for a broad range of multi-tier applications. We demonstrate the effective use of our model through experiments that analyze the behavior of an online auction application using a common workload benchmark.},
author = {Nanda, Saurav and Hacker, Thomas J. and Lu, Yung Hsiang},
doi = {10.1109/CloudCom.2016.0059},
file = {:E\:/文章/Predictive Model for Dynamically Provisioning Resources in Multi-Tier Web Applications.pdf:pdf},
isbn = {9781509014453},
issn = {23302186},
journal = {Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom},
keywords = {ARIMA,Bandpass Filter,Dynamic Resource Provisioning,Feedback Control System,Multi-tier Applications},
pages = {326--335},
title = {{Predictive model for dynamically provisioning resources in multi-tier web applications}},
volume = {0},
year = {2016}
}
@misc{TraefikLab2021,
author = {TraefikLab},
title = {{Traefik : Edge Router}},
url = {https://doc.traefik.io/traefik/.},
year = {2021}
}
@article{Li2018a,
abstract = {In recent years, the LXC (Linux Container)-based Docker technology has attracted great attention due to its lightweight and convenient features. Because of the performance differences among the hosts and the network complexity, it's necessary to improve the resource utilization of WEB application clusters. This paper analyzes the WEB application deployment architecture based on container technology, calculates the host load metrics combined the host's performance indicates and status of the operating containers, including CPU utilization, memory utilization, network utilization, and the proportion of unused memory that has been allocated for the containers, and proposes a Dynamic Weighted Least-Connection Algorithm(DWLC). The experiment shows that, the DWLC algorithm helps the WEB application response 52.6% and 46.4% faster than the ordinary Round-Robin and Least-Connection algorithm.},
author = {Li, Ruoyu and Li, Yunchun and Li, Wei},
doi = {10.1088/1742-6596/1060/1/012078},
file = {:E\:/文章/An Integrated Load-balancing Scheduling Algorithm for Nginx-Based Web Application Clusters.pdf:pdf},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
number = {1},
title = {{An Integrated Load-balancing Scheduling Algorithm for Nginx-Based Web Application Clusters}},
volume = {1060},
year = {2018}
}
@book{Buyya2019,
abstract = {A comprehensive guide to Fog and Edge applications, architectures, and technologies Recent years have seen the explosive growth of the Internet of Things (IoT): the internet-connected network of devices that includes everything from personal electronics and home appliances to automobiles and industrial machinery. Responding to the ever-increasing bandwidth demands of the IoT, Fog and Edge computing concepts have developed to collect, analyze, and process data more efficiently than traditional cloud architecture. Fog and Edge Computing: Principles and Paradigms provides a comprehensive overview of the state-of-the-art applications and architectures driving this dynamic field of computing while highlighting potential research directions and emerging technologies. Exploring topics such as developing scalable architectures, moving from closed systems to open systems, and ethical issues rising from data sensing, this timely book addresses both the challenges and opportunities that Fog and Edge computing presents. Contributions from leading IoT experts discuss federating Edge resources, middleware design issues, data management and predictive analysis, smart transportation and surveillance applications, and more. A coordinated and integrated presentation of topics helps readers gain thorough knowledge of the foundations, applications, and issues that are central to Fog and Edge computing. This valuable resource: • Provides insights on transitioning from current Cloud-centric and 4G/5G wireless environments to Fog Computing • Examines methods to optimize virtualized, pooled, and shared resources • Identifies potential technical challenges and offers suggestions for possible solutions • Discusses major components of Fog and Edge computing architectures such as middleware, interaction protocols, and autonomic management • Includes access to a website portal for advanced online resources Fog and Edge Computing: Principles and Paradigms is an essential source of up-to-date information for systems architects, developers, researchers, and advanced undergraduate and graduate students in fields of computer science and engineering.},
author = {Buyya, Rajkumar and Srirama, Satish Narayana},
booktitle = {Fog and Edge Computing: Principles and Paradigms},
doi = {10.1002/9781119525080},
isbn = {9781119525080},
pages = {1--471},
publisher = {John Wiley & Sons, Ltd},
title = {{Fog and edge computing: Principles and paradigms}},
year = {2019}
}
@article{Liu2019,
abstract = {Resource scheduling algorithms are crucial for cloud web applications to minimize resource rental costs of virtual machines elastically rented from public clouds while guaranteeing performances. Most existing algorithms only focus on virtual machines with fixed prices without considering spot virtual machines which have dynamic and cheaper prices. A few existing algorithms have considered the renting of spot resources to decrease rental costs. However, trends of dynamic prices are not considered which are beneficial to decrease rental costs further. Meanwhile, although spot virtual machines are cheaper, fluctuated prices are likely to produce out-of-bid events which make spot virtual machines unreliable. In this paper, a spot price prediction and minimum renting interval based resource scheduling method is proposed. Spot prices are predicted to identify cheap and stable spot types to improve the reliability. Frequent spot type switching is avoided in some extent by setting a minimum renting interval. The proposals are evaluated on CloudSim and experimental results show a better performance than existing algorithms.},
author = {Liu, Duan and Cai, Zhicheng and Lu, Yifei},
doi = {10.1109/CBD.2019.00024},
file = {:E\:/文章/Spot Price Prediction based Dynamic Resource.pdf:pdf},
isbn = {9781728151403},
journal = {Proceedings - 2019 7th International Conference on Advanced Cloud and Big Data, CBD 2019},
keywords = {Cloud computing,Price prediction,Resource scheduling,Spot instances},
pages = {78--83},
title = {{Spot price prediction based dynamic resource scheduling for web applications}},
year = {2019}
}
@article{Abdullah2020,
abstract = {Containers provide a lightweight runtime environment for microservices applications while enabling better server utilization. Automatic optimal allocation of CPU pins to the containers serving specific workloads can help to minimize the completion time of jobs. Most of the existing state-of-the-art focused on building new efficient scheduling algorithms for placing the containers on the infrastructure, and the resources to the containers are allocated manually and statically. An automatic method to identify and allocate optimal CPU resources to the containers can help to improve the efficiency of the scheduling algorithms. In this article, we introduce a new deep learning-based approach to allocate optimal CPU resources to the containers automatically. Our approach uses the law of diminishing marginal returns to determine the optimal number of CPU pins for containers to gain maximum performance while maximizing the number of concurrent jobs. The proposed method is evaluated using real workloads on a Docker-based containerized infrastructure. The results demonstrate the effectiveness of the proposed solution in reducing the completion time of the jobs by 23% to 74% compared to commonly used static CPU allocation methods.},
author = {Abdullah, Muhammad and Iqbal, Waheed and Bukhari, Faisal and Erradi, Abdelkarim},
doi = {10.1109/TNSM.2020.3033025},
file = {:E\:/文章/Diminishing Returns and Deep Learning for.pdf:pdf},
issn = {19324537},
journal = {IEEE Transactions on Network and Service Management},
keywords = {CPU allocation,CPU pin,Containers,diminishing marginal,job completion time},
number = {4},
pages = {2052--2063},
title = {{Diminishing Returns and Deep Learning for Adaptive CPU Resource Allocation of Containers}},
volume = {17},
year = {2020}
}
@article{Hegde2017,
abstract = {Operating system (OS) containers provide a process level virtualization in a multi-tenant Cloud environment. Such containers are becoming increasingly popular in developer community as they facilitate fast development and delivery of enterprise class Cloud services. Furthermore, these containers share a common OS and hence, they have a low resource foot-print leading to reduced provisioning time. In this paper, we investigate such promise of containers while provisioning large scale 3-tier applications. First, through benchmarking, we observe that at very large scale, several application scaling factors (e.g., number of containers of an application provisioned in parallel, application load) and system state parameters (e.g., number of applications and containers running on a system) introduce variability in application provisioning time because of resource bottleneck in general, and specifically due to the OS process overhead. To address such variability, we propose a provisioning decision management system SCoPe that provides an application partitioning and provisioning strategy where we determine the maximum number of containers of every application that can be provisioned in parallel across physical machines, while meeting the service level agreement (SLA) on provisioning time and Cloud provider specific objectives (e.g., maximize consolidation of applications, minimize operating cost). This joint partitioning and provisioning problem is NP-hard and we propose a greedy heuristic solution. Using real data set and through extensive experiments, we demonstrate the performance of SCoPe for large scale container based application provisioning. Compared to other well-known heuristics, SCoPe can reduce the partitioning by 5x or more, while meeting SLAs.},
author = {Hegde, Aditya and Ghosh, Rahul and Mukherjee, Tridib and Sharma, Varun},
doi = {10.1109/CLOUD.2016.34},
file = {:E\:/文章/A Decision System for Large Scale Container Provisioning Management.pdf:pdf},
isbn = {9781509026197},
issn = {21596190},
journal = {IEEE International Conference on Cloud Computing, CLOUD},
pages = {220--227},
publisher = {IEEE},
title = {{SCoPe: A decision system for large scale container provisioning management}},
year = {2017}
}
@article{Zhang2016,
abstract = {Performance, in terms of quality of service and resource utilization, is one of top attractions in cloud. However, in practice, most multi-tier applications today frequently present large scale fluctuations of response time during periods of high resource utilization. It is hardly to find any reported work that elaborated the reasons causing response time fluctuations, especially when providing quality performance and high effective multi-tier systems in cloud environments. To this end, this paper, through extensive measurements of a multi-tier application benchmark (RUBiS), corroborates the existing of response time fluctuations. In addition, arithmetic mean of response time is not suitable for the measurement of multi-tier services system performance. In light of probing analysis of requests, we show that the large scale response time fluctuations can be caused by concurrent long or mix transactions. The numerical results validate the correctness of our findings. Consequently, an effective scheduling policy called CTP (cross-tier-proportion) is developed to smooth response time fluctuations while maintaining high resource utilization in the system.},
address = {NLD},
author = {Zhang, Wenbin and Shi, Yuliang and Liu, Lei and Zhang, Shidong and Zheng, Yongqing and Cui, Lizhen and Yu, Han},
doi = {10.1016/j.micpro.2016.05.017},
file = {:E\:/文章/CTP：A scheduling strategy to smooth response time fluctuations in.pdf:pdf},
issn = {01419331},
journal = {Microprocessors and Microsystems},
keywords = {Fine-grained analysis,Fluctuations,Response time,Scheduling policy},
month = {nov},
number = {PA},
pages = {198--208},
publisher = {Elsevier Science Publishers B. V.},
title = {{CTP: A scheduling strategy to smooth response time fluctuations in multi-tier website system}},
volume = {47},
year = {2016}
}
@inproceedings{Huang2017,
abstract = {With the rapid development of cloud computing in recent years, more and more individuals and corporations use cloud computing platform to deploy their web applications, which can significantly minimize their deployment costs. However, it is observed that the number of accesses to some web application often fluctuates over time, resulting in the so-called peakvalley phenomenon: The amount of reserved resources is often proportional to the peak need of physical resources, while most of the time the amount of required resources is far below the peak load and thus physical servers will be idle for most of the time. To solve this problem, we establish a queuing model M/M/C, which represents infinite source and multi-service window. Based on this queueing model, we can accurately predict the arrival time of each customer, which enables us to calculate the minimum amount of resources that meet the resource needs. Then, we use heuristic algorithms and dynamic programming method to design a Virtual Machine (VM) auto-scaling strategies, including horizontal scaling and vertical scaling. With the proposed model and scaling algorithms, we can make web applications not only meet customer needs, but also use the least amount of resources, improving the resource utilization and minimizing deployment costs. With extensive experiments, we show the proposed model and scaling algorithms can greatly improve resource utilization without sacrificing web application performance.},
author = {Huang, Gaopan and Wang, Songyun and Zhang, Mingming and Li, Yefei and Qian, Zhuzhong and Chen, Yuan and Zhang, Sheng},
booktitle = {2016 3rd International Conference on Systems and Informatics, ICSAI 2016},
doi = {10.1109/ICSAI.2016.7810994},
isbn = {9781509055210},
keywords = {Auto-scale,Cloud-computing,Web application},
pages = {433--438},
title = {{Auto scaling virtual machines for web applications with queueing theory}},
year = {2017}
}
@article{Guerrero2017,
abstract = {The use of containers in cloud architectures has become widespread, owing to advantages such as limited overheads, easier and faster deployment, and higher portability. Moreover, they present a suitable architectural solution for the deployment of applications created using a microservice development pattern. Despite the large number of solutions and implementations, there remain open issues that have not been completely addressed in container automation and management. Container resource allocation influences system performance and resource consumption, and so it is a key factor for cloud providers. We propose a genetic algorithm approach, using the Non-dominated Sorting Genetic Algorithm- II (NSGA-II), to optimize container allocation and elasticity management, motivated by the good results obtained with this algorithm in other resource management optimization problems in cloud architectures. Our optimization algorithm enhances system provisioning, system performance, system failure, and network overhead. A model for cloud clusters, containers, microservices, and four optimization objectives is presented. Experimental results demonstrate that our approach is a suitable solution for addressing the problem of container allocation and elasticity, and it obtains better objective values than the container management policies implemented in Kubernetes.},
author = {Guerrero, Carlos and Lera, Isaac and Juiz, Carlos},
doi = {10.1007/s10723-017-9419-x},
file = {:E\:/文章/Genetic algorithm for multi-objective optimization of container allocation in__cloud architecture.pdf:pdf},
issn = {15729184},
journal = {Journal of Grid Computing},
keywords = {Cloud containers,Genetic algorithm,Microservices,Multi-objective optimization,Performance evaluation,Resource allocation},
number = {1},
pages = {113--135},
publisher = {Journal of Grid Computing},
title = {{Genetic algorithm for multi-objective optimization of container allocation in cloud architecture}},
volume = {16},
year = {2017}
}
@article{Xie2020,
abstract = {In recent years, there are increasing technologies that benefit from cloud computing and edge computing, especially the application in the Deep Learning and Internet of Things topics. Functionalities such as load balancing, traffic routing, and task scheduling, which used to be part of the software, are now enabled and provided as API or microservices on the underlying infrastructure. Therefore, when we want to deliver a container application to the users either on the cloud cluster or edge cluster, developers do not need to worry about these issues and only focus on the development of the functionality. Nevertheless, for the deployment stage, the system architect should design the fundamental architecture to build an ecosystem for the application. In this paper, a practical approach is proposed to develop a flexible and on-demand system for deep learning applications based on container and service mesh technologies. Our experiment chooses Kubernetes as the container orchestration platform and introduces Istio to implement the service mesh. We packaged one Flask based Deep learning model into a Docker container, and successfully deploy and provision this image classifier application to the public users with Functionalities like load balancing and task scheduling. Furthermore, we empower the system with an evaluation of the resource utilization services, and traffic flows inside the Kubernetes cluster in a visualized manner.},
author = {Xie, Xiaojing and Govardhan, Shyam S.},
doi = {10.1109/CCGrid49817.2020.00009},
file = {:E\:/文章/A Service Mesh-Based Load Balancing and Task.pdf:pdf},
isbn = {9781728160955},
journal = {Proceedings - 20th IEEE/ACM International Symposium on Cluster, Cloud and Internet Computing, CCGRID 2020},
keywords = {Docker container,Kubernetes,deep learning,load balancing,service mesh,task scheduling},
pages = {843--849},
title = {{A Service Mesh-Based Load Balancing and Task Scheduling System for Deep Learning Applications}},
year = {2020}
}
@article{Vilaplana2014,
abstract = {The ability to deliver guaranteed QoS (Quality of Service) is crucial for the commercial success of cloud platforms. This paper presents a model based on queuing theory to study computer service QoS in cloud computing. Cloud platforms are modeled with an open Jackson network that can be used to determine and measure the QoS guarantees the cloud can offer regarding the response time. The analysis can be performed according to different parameters, such as the arrival rate of customer services and the number and service rate of processing servers, among others. Detailed results for the model are presented. When scaling the system and depending on the types of bottleneck in the system, we show how our model can provide us with the best option to guarantee QoS. The results obtained confirm the usefulness of the model presented for designing real cloud computing systems. {\textcopyright} 2014 Springer Science+Business Media New York.},
author = {Vilaplana, Jordi and Solsona, Francesc and Teixid{\'{o}}, Ivan and Mateo, Jordi and Abella, Francesc and Rius, Josep},
doi = {10.1007/s11227-014-1177-y},
file = {:E\:/文章/A queuing theory model for cloud computing.pdf:pdf},
isbn = {1122701411},
issn = {15730484},
journal = {Journal of Supercomputing},
keywords = {Cloud architecture,Cloud computing,Quality of Service,Queuing theory,Scalability,Simulation,Validation},
number = {1},
pages = {492--507},
title = {{A queuing theory model for cloud computing}},
volume = {69},
year = {2014}
}
@article{Rahman2018,
abstract = {Virtualization of network functions (as virtual routers, virtual firewalls, etc.) enables network owners to efficiently respond to the increasing dynamicity of network services. Virtual Network Functions (VNFs) are easy to deploy, update, monitor, and manage. The number of VNF instances, similar to generic computing resources in cloud, can be easily scaled based on load. Hence, auto-scaling (of resources without human intervention) has been receiving attention. Prior studies on autoscaling use measured network traffic load to dynamically react to traffic changes. In this study, we propose a proactive Machine Learning (ML) based approach to perform auto-scaling of VNFs in response to dynamic traffic changes. Our proposed ML classifier learns from past VNF scaling decisions and seasonal/spatial behavior of network traffic load to generate scaling decisions ahead of time. Compared to existing approaches for ML-based auto-scaling, our study explores how the properties (e.g., startup time) of underlying virtualization technology impacts Quality of Service (QoS) and cost savings. We consider four different virtualization technologies: Xen and KVM, based on hypervisor virtualization, and Docker and LXC, based on container virtualization. Our results show promising accuracy of the ML classifier using real data collected from a private ISP. We report indepth analysis of the learning process (learning-curve analysis), feature ranking (feature selection, Principal Component Analysis (PCA), etc.), impact of different sets of features, training time, and testing time. Our results show how the proposed methods improve QoS and reduce operational cost for network owners. We also demonstrate a practical use-case example (Software-Defined Wide Area Network (SD-WAN) with VNFs and backbone network) to show that our ML methods save significant cost for network service leasers.},
archivePrefix = {arXiv},
arxivId = {1808.02975},
author = {Rahman, Sabidur and Ahmed, Tanjila and Huynh, Minh and Tornatore, Massimo and Mukherjee, Biswanath},
eprint = {1808.02975},
file = {:E\:/文章/Auto-scaling VNFs using Machine Learning to.pdf:pdf},
isbn = {9781538631805},
issn = {23318422},
journal = {arXiv},
keywords = {Auto-Scaling,Backbone network,Cost savings,Machine learning,QoS,Virtual network functions},
publisher = {IEEE},
title = {{Auto-scaling network resources using machine learning to improve QoS and reduce cost}},
year = {2018}
}
@article{Al-Dhuraibi2018,
abstract = {Elasticity is a fundamental property in cloud computing that has recently witnessed major developments. This article reviews both classical and recent elasticity solutions and provides an overview of containerization, a new technological trend in lightweight virtualization. It also discusses major issues and research challenges related to elasticity in cloud computing. We comprehensively review and analyze the proposals developed in this field. We provide a taxonomy of elasticity mechanisms according to the identified works and key properties. Compared to other works in literature, this article presents a broader and detailed analysis of elasticity approaches and is considered as the first survey addressing the elasticity of containers.},
author = {Al-Dhuraibi, Yahya and Paraiso, Fawaz and Djarallah, Nabil and Merle, Philippe},
doi = {10.1109/TSC.2017.2711009},
file = {:E\:/文章/Elasticity in Cloud Computing State of the Art and Research Challenges.pdf:pdf},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Elasticity,auto-scaling,cloud computing,containers,resource provision,scalability},
number = {2},
pages = {430--447},
title = {{Elasticity in Cloud Computing: State of the Art and Research Challenges}},
volume = {11},
year = {2018}
}
@article{Gai2021,
abstract = {Combining the Internet-of-Things (IoT) technology with cloud computing is a significant alternative for powering the utilization of computing resources in a connected environment. A grand challenge in communications is raised by the emergence of big data, due to the large-sized data transmissions and frequent data exchanges. Applying fog computing is considered an option for resolving the communication challenge. However, a high extent of available heterogeneous computing attached to fog computing servers leads to a restriction of the resource management. This Article addresses the resource management issue by proposing a novel approach-named Energy-aware Fog Resource Optimization (EFRO) model-to optimizing the utilization of connected devices in fog computing. We develop a heuristic algorithm minimizing both energy cost and time consumption in a holistic way. A salient feature of EFRO lies in the integration of the standardization and smart shift operations fueled by a hill-climbing mechanism to produce near-optimal resource allocation solutions. Experimental results demonstrate that our EFRO is adroit at making near-optimal decisions in managing resources in fog computing environments. In particular, EFRO boosts the energy efficiency of the existing MESF and RR schemes by 54.83 and 71.28 percent, respectively. EFRO shortens DECM's allocation-generation time by up to a factor of 507.},
author = {Gai, Keke and Qin, Xiao and Zhu, Liehuang},
doi = {10.1109/TC.2020.2993561},
file = {:E\:/文章/An Energy-aware High Performance Task__Allocation Strategy in Heterogeneous Fog__Computing Environments.pdf:pdf},
issn = {15579956},
journal = {IEEE Transactions on Computers},
keywords = {Fog computing,allocation strategy,energy-aware,heterogeneous computing,resource management},
number = {4},
pages = {626--639},
title = {{An Energy-Aware High Performance Task Allocation Strategy in Heterogeneous Fog Computing Environments}},
volume = {70},
year = {2021}
}
@article{Littley2019,
abstract = {Docker container images are typically stored in a centralized registry to allow easy sharing of images. However, with the growing popularity of containerized software, the number of images that a registry needs to store and the rate of requests it needs to serve are increasing rapidly. Current registry design requires hosting registry services across multiple loosely connected servers with different roles such as load balancers, proxies, registry servers, and object storage servers. Due to the various individual components, registries are hard to scale and benefits from optimizations such as caching are limited. In this paper we propose, implement, and evaluate BOLT-a new hyperconverged design for container registries. In BOLT, all registry servers are part of a tightly connected cluster and play the same consolidated role: each registry server caches images in its memory, stores images in its local storage, and provides computational resources to process client requests. The design employs a custom consistent hashing function to take advantage of the layered structure and addressing of images and to load balance requests across different servers. Our evaluation using real production workloads shows that BOLT outperforms the conventional registry design significantly and improves latency by an order of magnitude and throughput by up to 5x. Compared to state-of-the-art, BOLT can utilize cache space more efficiently and serve up to 35% more requests from its cache. Furthermore, BOLT scales linearly and recovers from failure recovery without significant performance degradation.},
author = {Littley, Michael and Anwar, Ali and Fayyaz, Hannan and Fayyaz, Zeshan and Tarasov, Vasily and Rupprecht, Lukas and Skourtis, Dimitrios and Mohamed, Mohamed and Ludwig, Heiko and Cheng, Yue and Butt, Ali R.},
doi = {10.1109/CLOUD.2019.00065},
file = {:E\:/文章/Bolt：Towards a Scalable Docker Registry via.pdf:pdf},
isbn = {9781728127057},
issn = {21596190},
journal = {IEEE International Conference on Cloud Computing, CLOUD},
keywords = {Container distribution,Container performance,Docker Registry,Docker design,Docker performance,hyper converged infrastructure},
pages = {358--366},
publisher = {IEEE},
title = {{Bolt: Towards a scalable docker registry via hyperconvergence}},
volume = {2019-July},
year = {2019}
}
@inproceedings{KhoLin2019,
abstract = {The Australian Defence Forces (ADF) including the army, navy and air force often share resources. This is the case for helicopter training where the resources are often people, e.g. instructors. Understanding the current and future needs of the helicopter pilot training continuum is challenging. New students are continually entering the Defence services as trainees/cadets; passing or failing exams at specialist flying schools; progressing through or leaving the Defence services and potentially becoming trained pilots or instructors that can subsequently train future students. This continuum has an associated optimisation challenge. The ATHENA platform has been developed as a strategic discrete event simulation, optimisation and analysis system for manpower planning with specific focus on addressing the needs and demands of this helicopter pilot training continuum for the ADF. ATHENA has been developed as a simulation platform running on a Cloud infrastructure. This paper introduces ATHENA and describes the way in which the platform leverages container technologies to auto-scale across the Cloud with focus on the Kubernetes orchestration technology.},
author = {{Kho Lin}, S and Altaf, Umer and Jayaputera, Glenn and Li, Jiajie and Marques, Davis and Meggyesy, David and Sarwar, Sulman and Sharma, Shivank and Voorsluys, William and Sinnott, Richard and Novak, Ana and Nguyen, Vivian and Pash, Kristan},
booktitle = {Proceedings - 11th IEEE/ACM International Conference on Utility and Cloud Computing Companion, UCC Companion 2018},
doi = {10.1109/UCC-Companion.2018.00076},
isbn = {9781728103594},
keywords = {Auto-scaling,Container orchestration,Docker,Kubernetes,OpenStack},
pages = {327--334},
title = {{Auto-Scaling a Defence Application across the Cloud Using Docker and Kubernetes}},
year = {2019}
}
@article{Al-Dhuraibi2018a,
author = {Al-Dhuraibi, Y and Paraiso, F and Djarallah, N and Merle, P},
doi = {10.1109/TSC.2017.2711009},
file = {:E\:/文章/Elasticity in Cloud Computing：State of the.pdf:pdf},
journal = {IEEE Transactions on Services Computing},
number = {2},
pages = {430--447},
title = {{Elasticity in Cloud Computing: State of the Art and Research Challenges}},
volume = {11},
year = {2018}
}
@article{Adam2017,
abstract = {Under today's bursty web traffic, the fine-grained per-container control promises more efficient resource provisioning for web services and better resource utilization in cloud datacenters. In this paper, we present Two-stage Stochastic Programming Resource Allocator (2SPRA). It optimizes resource provisioning for containerized n-tier web services in accordance with fluctuations of incoming workload to accommodate predefined SLOs on response latency. In particular, 2SPRA is capable of minimizing resource over-provisioning by addressing dynamics of web traffic as workload uncertainty in a native stochastic optimization model. Using special-purpose OpenOpt optimization framework, we fully implement 2SPRA in Python and evaluate it against three other existing allocation schemes, in a Docker-based CoreOS Linux VMs on Amazon EC2. We generate workloads based on four real-world web traces of various traffic variations: AOL, WorldCup98, ClarkNet, and NASA. Our experimental results demonstrate that 2SPRA achieves the minimum resource over-provisioning outperforming other schemes. In particular, 2SPRA allocates only 6.16 percent more than application's actual demand on average and at most 7.75 percent in the worst case. It achieves 3x further reduction in total resources provisioned compared to other schemes delivering overall cost-savings of 53.6 percent on average and up to 66.8 percent. Furthermore, 2SPRA demonstrates consistency in its provisioning decisions and robust responsiveness against workload fluctuations.},
author = {Adam, Omer and Lee, Young Choon and Zomaya, Albert Y},
doi = {10.1109/TPDS.2016.2639009},
file = {:E\:/文章/Stochastic Resource Provisioning for.pdf:pdf},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Cloud resources provisioning,Containers,Multi-tier web applications,Stochastic optimization,Workload uncertainty},
number = {7},
pages = {2060--2073},
title = {{Stochastic resource provisioning for containerized multi-tier web services in clouds}},
volume = {28},
year = {2017}
}
@article{Zhong2020,
abstract = {Containers, as a lightweight application virtualization technology, have recently gained immense popularity in mainstream cluster management systems like Google Borg and Kubernetes. Prevalently adopted by these systems for task deployments of diverse workloads such as big data, web services, and IoT, they support agile application deployment, environmental consistency, OS distribution portability, application-centric management, and resource isolation. Although most of these systems are mature with advanced features, their optimization strategies are still tailored to the assumption of a static cluster. Elastic compute resources would enable heterogeneous resource management strategies in response to the dynamic business volume for various types of workloads. Hence, we propose a heterogeneous task allocation strategy for cost-efficient container orchestration through resource utilization optimization and elastic instance pricing with three main features. The first one is to support heterogeneous job configurations to optimize the initial placement of containers into existing resources by task packing. The second one is cluster size adjustment to meet the changing workload through autoscaling algorithms. The third one is a rescheduling mechanism to shut down underutilized VM instances for cost saving and reallocate the relevant jobs without losing task progress. We evaluate our approach in terms of cost and performance on the Australian National Cloud Infrastructure (Nectar). Our experiments demonstrate that the proposed strategy could reduce the overall cost by 23% to 32% for different types of cloud workload patterns when compared to the default Kubernetes framework.},
address = {New York, NY, USA},
author = {Zhong, Zhiheng and Buyya, Rajkumar},
doi = {10.1145/3378447},
file = {:E\:/文章/k8s.pdf:pdf},
issn = {15576051},
journal = {ACM Transactions on Internet Technology},
keywords = {Cluster management,container orchestration,cost efficiency,resource heterogeneity},
month = {apr},
number = {2},
publisher = {Association for Computing Machinery},
title = {{A Cost-Efficient Container Orchestration Strategy in Kubernetes-Based Cloud Computing Infrastructures with Heterogeneous Resources}},
volume = {20},
year = {2020}
}
@article{Adam2017a,
abstract = {Under today's bursty web traffic, the fine-grained per-container control promises more efficient resource provisioning for web services and better resource utilization in cloud datacenters. In this paper, we present Two-stage Stochastic Programming Resource Allocator (2SPRA). It optimizes resource provisioning for containerized n-tier web services in accordance with fluctuations of incoming workload to accommodate predefined SLOs on response latency. In particular, 2SPRA is capable of minimizing resource over-provisioning by addressing dynamics of web traffic as workload uncertainty in a native stochastic optimization model. Using special-purpose OpenOpt optimization framework, we fully implement 2SPRA in Python and evaluate it against three other existing allocation schemes, in a Docker-based CoreOS Linux VMs on Amazon EC2. We generate workloads based on four real-world web traces of various traffic variations: AOL, WorldCup98, ClarkNet, and NASA. Our experimental results demonstrate that 2SPRA achieves the minimum resource over-provisioning outperforming other schemes. In particular, 2SPRA allocates only 6.16 percent more than application's actual demand on average and at most 7.75 percent in the worst case. It achieves 3x further reduction in total resources provisioned compared to other schemes delivering overall cost-savings of 53.6 percent on average and up to 66.8 percent. Furthermore, 2SPRA demonstrates consistency in its provisioning decisions and robust responsiveness against workload fluctuations.},
author = {Adam, Omer and Lee, Young Choon and Zomaya, Albert Y.},
doi = {10.1109/TPDS.2016.2639009},
file = {:E\:/文章/Stochastic Resource Provisioning for Containerized Multi-Tier Web Services in Clouds.pdf:pdf},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Cloud resources provisioning,Containers,Multi-tier web applications,Stochastic optimization,Workload uncertainty},
number = {7},
pages = {2060--2073},
publisher = {IEEE},
title = {{Stochastic resource provisioning for containerized multi-tier web services in clouds}},
volume = {28},
year = {2017}
}
@article{Calheiros2011,
abstract = {Cloud computing is a recent advancement wherein IT infrastructure and applications are provided as 'services' to end-users under a usage-based payment model. It can leverage virtualized services even on the fly based on requirements (workload patterns and QoS) varying with time. The application services hosted under Cloud computing model have complex provisioning, composition, configuration, and deployment requirements. Evaluating the performance of Cloud provisioning policies, application workload models, and resources performance models in a repeatable manner under varying system and user configurations and requirements is difficult to achieve. To overcome this challenge, we propose CloudSim: an extensible simulation toolkit that enables modeling and simulation of Cloud computing systems and application provisioning environments. The CloudSim toolkit supports both system and behavior modeling of Cloud system components such as data centers, virtual machines (VMs) and resource provisioning policies. It implements generic application provisioning techniques that can be extended with ease and limited effort. Currently, it supports modeling and simulation of Cloud computing environments consisting of both single and inter-networked clouds (federation of clouds). Moreover, it exposes custom interfaces for implementing policies and provisioning techniques for allocation of VMs under inter-networked Cloud computing scenarios. Several researchers from organizations, such as HP Labs in U.S.A., are using CloudSim in their investigation on Cloud resource provisioning and energy-efficient management of data center resources. The usefulness of CloudSim is demonstrated by a case study involving dynamic provisioning of application services in the hybrid federated clouds environment. The result of this case study proves that the federated Cloud computing model significantly improves the application QoS requirements under fluctuating resource and service demand patterns. Copyright {\textcopyright} 2010 John Wiley & Sons, Ltd.},
author = {Calheiros, Rodrigo N and Ranjan, Rajiv and Beloglazov, Anton and {De Rose}, C{\'{e}}sar A.F. and Buyya, Rajkumar},
doi = {10.1002/spe.995},
issn = {00380644},
journal = {Software - Practice and Experience},
keywords = {Cloud computing,application scheduling,modelling and simulation,performance evaluation,resource management},
month = {jan},
number = {1},
pages = {23--50},
title = {{CloudSim: A toolkit for modeling and simulation of cloud computing environments and evaluation of resource provisioning algorithms}},
volume = {41},
year = {2011}
}
@article{Gai2019,
abstract = {The interest in fog computing is growing, including in traditionally conservative and sensitive areas such as military and governments. This is partly driven by the interconnectivity of our society, and advances in technologies such as Internet-of-Things (IoT). However, protecting against privacy leakage is one of several key considerations in fog computing deployment. Therefore, in this paper, we present a privacy- preserving multi-layer access filtering model, designed for a fog computing environment; hence, coined fog-based access filter (FAF). FAF comprises three key algorithms, namely: access filter initialization algorithm, optimal privacy-energy-time algorithm, and tuple reduction algorithm. Also, a hierarchical classification is used to distinguish the protection objectives. Findings from our experimental evaluation demonstrate that FAF allows one to achieve an optimal balance between privacy protection and computational costs.},
author = {Gai, Keke and Zhu, Liehuang and Qiu, Meikang and Xu, Kai and Choo, Kim Kwang Raymond},
doi = {10.1109/TCC.2019.2942293},
file = {:E\:/文章/Multi-Access Filtering for Privacy-preserving Fog_ Computing.pdf:pdf},
issn = {21687161},
journal = {IEEE Transactions on Cloud Computing},
keywords = {Access control,Cloud computing,Computational modeling,Differential privacy,Edge computing,Privacy,Privacy-preserving,Servers,access filter,dynamic programming,fog computing,network fusion,optimal scheduling},
number = {c},
pages = {1--13},
publisher = {IEEE},
title = {{Multi-Access Filtering for Privacy-preserving Fog Computing}},
volume = {7161},
year = {2019}
}
@inproceedings{MarinescuDanC.2015,
abstract = {In this paper we discuss why cloud self-organization is not only desirable, but also critical for the future of cloud computing. We analyze major challenges and discuss practical principles for cloud self-organization. After a brief presentation of a hierarchical cloud architecture model we outline the advantages of a self-organization model based on coalition formation and combinatorial auctions.},
address = {Cham},
author = {{Marinescu Dan C.} and and Morrison, John P. and Ashkan, and Paya},
booktitle = {Adaptive Resource Management and Scheduling for Cloud Computing},
editor = {{Pop Florin} and and Potop-Butucaru, Maria},
isbn = {978-3-319-28448-4},
pages = {119--127},
publisher = {Springer International Publishing},
title = {{Is Cloud Self-organization Feasible?}},
year = {2015}
}
@inproceedings{Delnat2018,
abstract = {Although a considerable amount of research exists on auto-scaling of database clusters, the design of an effective auto-scaling strategy requires fine-grained tailoring towards the specific application scenario. This paper presents an easy-to-use and extensible workbench exemplar, named K8-Scalar (Kube-Scalar), which allows researchers to implement and evaluate different self-adaptive approaches to auto-scaling container-orchestrated services. The workbench is based on Docker, a popular technology for easing the deployment of containerized software that also has been positioned as an enabler for reproducible research. The workbench also relies on a container orchestration framework: Kubernetes (K8s), the de-facto industry standard for orchestration and monitoring of elastically scalable container-based services. Finally, it integrates and extends Scalar, a generic testbed for evaluating the scalability of large-scale systems with support for evaluating the performance of autoscalers for database clusters. The paper discusses (i) the architecture and implementation of K8-Scalar and how a particular autoscaler can be plugged in, (ii) sketches the design of a Riemann-based autoscaler for database clusters, (iii) illustrates how to design, setup and analyze a series of experiments to configure and evaluate the performance of this autoscaler for a particular database (i.e., Cassandra) and a particular workload type, and (iv) validates the effectiveness of K8-Scalar as a workbench for accurately comparing the performance of different auto-scaling strategies.},
author = {Delnat, Wito and Truyen, Eddy and Rafique, Ansar and {Van Landuyt}, Dimitri and Joosen, Wouter},
booktitle = {Proceedings - International Conference on Software Engineering},
doi = {10.1145/3194133.3194162},
file = {:E\:/文章/K8-Scalar：a workbench to compare autoscalers for.pdf:pdf},
isbn = {9781450357159},
issn = {02705257},
keywords = {autoscaling,container orchestration,experimentation exemplar},
pages = {33--39},
title = {{K8-scalar: A workbench to compare autoscalers for container-orchestrated database clusters}},
year = {2018}
}
@article{Salah2016,
abstract = {In the cloud, ensuring proper elasticity for hosted applications and services is a challenging problem and far from being solved. To achieve proper elasticity, the minimal number of cloud resources that are needed to satisfy a particular service level objective (SLO) requirement has to be determined. In this paper, we present an analytical model based on Markov chains to predict the number of cloud instances or virtual machines (VMs) needed to satisfy a given SLO performance requirement such as response time, throughput, or request loss probability. For the estimation of these SLO performance metrics, our analytical model takes the offered workload, the number of VM instances as an input, and the capacity of each VM instance. The correctness of the model has been verified using discrete-event simulation. Our model has also been validated using experimental measurements conducted on the Amazon Web Services cloud platform.},
author = {Salah, Khaled and Elbadawi, Khalid and Boutaba, Raouf},
doi = {10.1007/s10922-015-9352-x},
file = {:E\:/文章/An Analytical Model for Estimating Cloud Resources of Elastic Services.pdf:pdf},
issn = {10647570},
journal = {Journal of Network and Systems Management},
keywords = {Auto-scaling,Capacity engineering,Cloud computing,Elasticity,Performance modeling and analysis,Resource management},
number = {2},
pages = {285--308},
publisher = {Springer US},
title = {{An Analytical Model for Estimating Cloud Resources of Elastic Services}},
volume = {24},
year = {2016}
}
@misc{Qu2018a,
abstract = {Web application providers have been migrating their applications to cloud data centers, attracted by the emerging cloud computing paradigm. One of the appealing features of the cloud is elasticity. It allows cloud users to acquire or release computing resources on demand, which enables web application providers to automatically scale the resources provisioned to their applications without human intervention under a dynamic workload to minimize resource cost while satisfying Quality of Service (QoS) requirements. In this article, we comprehensively analyze the challenges that remain in auto-scaling web applications in clouds and review the developments in this field. We present a taxonomy of auto-scalers according to the identified challenges and key properties. We analyze the surveyed works and map them to the taxonomy to identify the weaknesses in this field. Moreover, based on the analysis, we propose new future directions that can be explored in this area.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {1609.09224},
author = {Qu, Chenhao and Calheiros, Rodrigo N and Buyya, Rajkumar},
booktitle = {ACM Computing Surveys},
doi = {10.1145/3148149},
eprint = {1609.09224},
file = {:E\:/文章/Auto-scaling web applications in clouds_ a taxono.pdf:pdf},
issn = {15577341},
keywords = {Auto-scaling,Cloud computing,Web application},
month = {jul},
number = {4},
publisher = {Association for Computing Machinery},
title = {{Auto-scaling web applications in clouds: A taxonomy and survey}},
volume = {51},
year = {2018}
}
@misc{,
title = {{Kubernetes java client}},
url = {https://github.com/kubernetes-client/java},
year = {2021}
}
@article{Piraghaj2017,
abstract = {Containers are increasingly gaining popularity and becoming one of the major deployment models in cloud environments. To evaluate the performance of scheduling and allocation policies in containerized cloud data centers, there is a need for evaluation environments that support scalable and repeatable experiments. Simulation techniques provide repeatable and controllable environments, and hence, they serve as a powerful tool for such purpose. This paper introduces ContainerCloudSim, which provides support for modeling and simulation of containerized cloud computing environments. We developed a simulation architecture for containerized clouds and implemented it as an extension of CloudSim. We described a number of use cases to demonstrate how one can plug in and compare their container scheduling and provisioning policies in terms of energy efficiency and SLA compliance. Our system is highly scalable as it supports simulation of large number of containers, given that there are more containers than virtual machines in a data center. Copyright {\textcopyright} 2016 John Wiley & Sons, Ltd.},
author = {Piraghaj, Sareh Fotuhi and Dastjerdi, Amir Vahid and Calheiros, Rodrigo N and Buyya, Rajkumar},
doi = {10.1002/spe.2422},
issn = {1097024X},
journal = {Software - Practice and Experience},
keywords = {cloud computing,container as a service (CaaS),containerized clouds,simulation},
number = {4},
pages = {505--521},
title = {{ContainerCloudSim: An environment for modeling and simulation of containers in cloud data centers}},
volume = {47},
year = {2017}
}
@article{Barrett2013,
abstract = {Public Infrastructure as a Service (IaaS) clouds such as Amazon, GoGrid and Rackspace deliver computational resources by means of virtualisation technologies. These technologies allow multiple independent virtual machines to reside in apparent isolation on the same physical host. Dynamically scaling applications running on IaaS clouds can lead to varied and unpredictable results because of the performance interference effects associated with co-located virtual machines. Determining appropriate scaling policies in a dynamic non-stationary environment is non-trivial. One principle advantage exhibited by IaaS clouds over their traditional hosting counterparts is the ability to scale resources on-demand. However, a problem arises concerning resource allocation as to which resources should be added and removed when the underlying performance of the resource is in a constant state of flux. Decision theoretic frameworks such as Markov Decision Processes are particularly suited to decision making under uncertainty. By applying a temporal difference, reinforcement learning algorithm known as Q-learning, optimal scaling policies can be determined. Additionally, reinforcement learning techniques typically suffer from curse of dimensionality problems, where the state space grows exponentially with each additional state variable. To address this challenge, we also present a novel parallel Q-learning approach aimed at reducing the time taken to determine optimal policies whilst learning online. Copyright {\textcopyright} 2012 John Wiley & Sons, Ltd.},
author = {Barrett, Enda and Howley, Enda and Duggan, Jim},
doi = {10.1002/cpe.2864},
issn = {15320626},
journal = {Concurrency Computation Practice and Experience},
keywords = {cloud computing,reinforcement learning,resource scaling},
number = {12},
pages = {1656--1674},
title = {{Applying reinforcement learning towards automating resource allocation and application scalability in the cloud}},
volume = {25},
year = {2013}
}
@article{Lorido-Botran2014,
abstract = {Cloud computing environments allow customers to dynamically scale their applications. The key problem is how to lease the right amount of resources, on a pay-as-you-go basis. Application re-dimensioning can be implemented effortlessly, adapting the resources assigned to the application to the incoming user demand. However, the identification of the right amount of resources to lease in order to meet the required Service Level Agreement, while keeping the overall cost low, is not an easy task. Many techniques have been proposed for automating application scaling. We propose a classification of these techniques into five main categories: static threshold-based rules, control theory, reinforcement learning, queuing theory and time series analysis. Then we use this classification to carry out a literature review of proposals for auto-scaling in the cloud.},
author = {Lorido-Botran, Tania and Miguel-Alonso, Jose and Lozano, Jose Antonio},
doi = {10.1007/s10723-014-9314-7},
issn = {15707873},
journal = {Journal of Grid Computing},
keywords = {Auto-scaling,Cloud computing,Scalable applications,Service level agreement},
number = {4},
pages = {559--592},
title = {{A Review of Auto-scaling Techniques for Elastic Applications in Cloud Environments}},
volume = {12},
year = {2014}
}
@inproceedings{Balaji2014,
abstract = {Cloud service providers, monitor average resource (for e.g. CPU) consumption and based on predefined limits (for e.g. CPU-Idle-time > 500 milliseconds), provision or de-provision resources. Traditionally this is a reactive approach and doesn't fully address the wide range of enterprise use cases. Implementation of predictive approach to resource management has been rarely reported even though they could perform potentially better than their counterpart. Identification of a suitable model for predicting the performance of the system under a load is an ideal precursor in managing resources on a cloud environment. The current study compares the performance of two such predictive models namely Holt-Winter and ARIMA using a public web server data set Request rate was used as the metric to monitor resource consumption. The experiment results show that Holt-Winter model performs better than a few selected ARIMA models, which could be subsequently used for managing resources on cloud if the data request rates follow a similar pattern. {\textcopyright} 2014 IEEE.},
author = {Balaji, Mahesh and Rao, G. Subrahmanya V.R.K. and Kumar, Ch Aswani},
booktitle = {Proceedings - 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2014},
doi = {10.1109/CCGrid.2014.32},
file = {:E\:/文章/A comparitive study of predictive models for cloud infrastructure management.pdf:pdf},
isbn = {9781479927838},
keywords = {ARIMA,Cloud computing,Holt-Winter,Predictive modeling,Resource management},
pages = {923--926},
title = {{A comparitive study of predictive models for cloud infrastructure management}},
year = {2014}
}
@article{Thai2018,
abstract = {Cloud computing has been widely adopted due to the flexibility in resource provisioning and on-demand pricing models. Entire clusters of Virtual Machines (VMs) can be dynamically provisioned to meet the computational demands of users. However, from a user's perspective, it is still challenging to utilise cloud resources efficiently. This is because an overwhelmingly wide variety of resource types with different prices and significant performance variations are available. This paper presents a survey and taxonomy of existing research in optimising the execution of Bag-of-Task applications on cloud resources. A BoT application consists of multiple independent tasks, each of which can be executed by a VM in any order; these applications are widely used by both the scientific communities and commercial organisations. The objectives of this survey are as follows: (i) to provide the reader with a concise understanding of existing research on optimising the execution of BoT applications on the cloud, (ii) to define a taxonomy that categorises current frameworks to compare and contrast them, and (iii) to present current trends and future research directions in the area.},
archivePrefix = {arXiv},
arxivId = {1711.08973},
author = {Thai, Long and Varghese, Blesson and Barker, Adam},
doi = {10.1016/j.future.2017.11.038},
eprint = {1711.08973},
file = {:E\:/文章/A Survey and Taxonomy of Resource Optimisation for Executing Bag-of-Task Applications on Public Clouds.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Bag-of-Task,Cloud computing,Execution optimisation,Resource usage optimisation},
number = {November},
pages = {1--11},
title = {{A survey and taxonomy of resource optimisation for executing bag-of-task applications on public clouds}},
volume = {82},
year = {2018}
}
@misc{AWS2019,
author = {AWS},
booktitle = {AWS},
title = {{Amazon EKS - Managed Kubernetes Service}},
url = {https://aws.amazon.com/eks/},
year = {2019}
}
@article{Huang2020,
abstract = {Job scheduling in cluster is often considered as a difficult online decision-making problem, and its solution depends largely on the understanding of the workload and environment. People usually first propose a simple heuristic scheduling algorithm, and then perform repeated and tedious manual tests and adjustments based on the characteristics of the workload to gradually improve the algorithm. In this work, focusing on multi-cluster environments, load balancing and efficient scheduling, we present RLSK, a deep reinforcement learning based job scheduler for scheduling independent batch jobs among multiple federated cloud computing clusters adaptively. By directly specifying high-level scheduling targets, RLSK interacts with the system environment and automatically learns scheduling strategies from experience without any prior knowledge assumed over the underlying multi-cluster environment and human instructions, which avoids people's tedious testing and tuning work. We implement our scheduler based on Kubernetes, and conduct simulations to evaluate the performance of our design. The results show that, RLSK can outperform traditional scheduling algorithms.},
author = {Huang, Jiaming and Xiao, Chuming and Wu, Weigang},
doi = {10.1109/IC2E48712.2020.00019},
file = {:E\:/文章/RLSK：A Job Scheduler for Federated Kubernetes.pdf:pdf},
isbn = {9781728110998},
journal = {Proceedings - 2020 IEEE International Conference on Cloud Engineering, IC2E 2020},
keywords = {Deep reinforcement learning,job scheduling,kubernetes,resource management},
pages = {116--123},
title = {{RLSK: A Job Scheduler for Federated Kubernetes Clusters based on Reinforcement Learning}},
year = {2020}
}
@article{Zhang2016a,
abstract = {Performance, in terms of quality of service and resource utilization, is one of top attractions in cloud. However, in practice, most multi-tier applications today frequently present large scale fluctuations of response time during periods of high resource utilization. It is hardly to find any reported work that elaborated the reasons causing response time fluctuations, especially when providing quality performance and high effective multi-tier systems in cloud environments. To this end, this paper, through extensive measurements of a multi-tier application benchmark (RUBiS), corroborates the existing of response time fluctuations. In addition, arithmetic mean of response time is not suitable for the measurement of multi-tier services system performance. In light of probing analysis of requests, we show that the large scale response time fluctuations can be caused by concurrent long or mix transactions. The numerical results validate the correctness of our findings. Consequently, an effective scheduling policy called CTP (cross-tier-proportion) is developed to smooth response time fluctuations while maintaining high resource utilization in the system.},
author = {Zhang, Wenbin and Shi, Yuliang and Liu, Lei and Zhang, Shidong and Zheng, Yongqing and Cui, Lizhen and Yu, Han},
doi = {10.1016/j.micpro.2016.05.017},
file = {:E\:/文章/CTP：A scheduling strategy to smooth response time fluctuations in.pdf:pdf},
issn = {01419331},
journal = {Microprocessors and Microsystems},
keywords = {Fine-grained analysis,Fluctuations,Response time,Scheduling policy},
pages = {198--208},
publisher = {Elsevier B.V.},
title = {{CTP: A scheduling strategy to smooth response time fluctuations in multi-tier website system}},
url = {http://dx.doi.org/10.1016/j.micpro.2016.05.017},
volume = {47},
year = {2016}
}
@article{Menouer2021,
abstract = {The Kubernetes framework is a well-known open-source container-orchestration system widely used in industrial and academic fields. In this paper, we introduce a new Kubernetes Container Scheduling Strategy called KCSS. The goal of the KCSS is to optimize the scheduling of several containers submitted online by users to improve the performance concerning the user need in terms of makespan and the cloud provider need in terms of power consumption. In the literature, several container scheduling strategies are proposed. Each one uses a dedicated approach to select one node from the node set supplied by the cloud infrastructure. Then, this node welcomes the newly submitted container. The majority of the proposed container scheduling strategies select for each newly submitted container a node based on a single criteria, such as the number of running containers or the amount of available resources in each node. However, the scheduling based on a single criterion downgrades the performance because the scheduler has a limited vision of the state of the cloud infrastructure and the user need. The contribution of our KCSS is to introduce a multi-criteria selection of the node. This approach provides the scheduler with a global vision about the state of the cloud and the user's need. The idea is to select from each newly submitted container the best node, with a good compromise between hybrid criteria related to the cloud infrastructure and the user need. In the KCSS, we consider six key criteria: (1) the CPUs utilization rate in each node; (2) the memory utilization rate in each node; (3) the disk utilization rate in each node; (4) the power consumption of each node; (5) the number of running containers in each node; and (6) the time of transmitting the image selected by the user for its container. In our context, the KCSS is based on a multi-criteria decision analysis algorithm to aggregate all criteria in a single rank. Then, select the node which has the highest rank to execute the new submitted container. The multi-criteria algorithm used by KCSS is the Technique for the Order of Prioritisation by Similarity to Ideal Solution (TOPSIS) algorithm. The KCSS is implemented in Go language inside the Kubernetes framework with minimal changes to be used easily with the next Kubernetes versions. The experimental results show that the KCSS improves the performance under different scenarios compared to other container scheduling strategies.},
author = {Menouer, Tarek},
doi = {10.1007/s11227-020-03427-3},
file = {:E\:/文章/Menouer2020_Article_KCSSKubernetesContainerSchedul.pdf:pdf},
issn = {15730484},
journal = {Journal of Supercomputing},
keywords = {Cloud computing,Container technology,Multi-criteria scheduling,On-line scheduling,Power consumption in scheduling,Scheduling strategy},
number = {5},
pages = {4267--4293},
publisher = {Springer US},
title = {{KCSS: Kubernetes container scheduling strategy}},
url = {https://doi.org/10.1007/s11227-020-03427-3},
volume = {77},
year = {2021}
}
@article{Huang2017a,
abstract = {With the rapid development of cloud computing in recent years, more and more individuals and corporations use cloud computing platform to deploy their web applications, which can significantly minimize their deployment costs. However, it is observed that the number of accesses to some web application often fluctuates over time, resulting in the so-called peakvalley phenomenon: The amount of reserved resources is often proportional to the peak need of physical resources, while most of the time the amount of required resources is far below the peak load and thus physical servers will be idle for most of the time. To solve this problem, we establish a queuing model M/M/C, which represents infinite source and multi-service window. Based on this queueing model, we can accurately predict the arrival time of each customer, which enables us to calculate the minimum amount of resources that meet the resource needs. Then, we use heuristic algorithms and dynamic programming method to design a Virtual Machine (VM) auto-scaling strategies, including horizontal scaling and vertical scaling. With the proposed model and scaling algorithms, we can make web applications not only meet customer needs, but also use the least amount of resources, improving the resource utilization and minimizing deployment costs. With extensive experiments, we show the proposed model and scaling algorithms can greatly improve resource utilization without sacrificing web application performance.},
author = {Huang, Gaopan and Wang, Songyun and Zhang, Mingming and Li, Yefei and Qian, Zhuzhong and Chen, Yuan and Zhang, Sheng},
doi = {10.1109/ICSAI.2016.7810994},
file = {:E\:/文章/Auto scaling virtual machines for web applications with queueing theory.pdf:pdf},
isbn = {9781509055210},
journal = {2016 3rd International Conference on Systems and Informatics, ICSAI 2016},
keywords = {Auto-scale,Cloud-computing,Web application},
number = {Icsai},
pages = {433--438},
publisher = {IEEE},
title = {{Auto scaling virtual machines for web applications with queueing theory}},
year = {2017}
}
@article{Guevara2020,
abstract = {Currently, Internet applications running on mobile devices generate a massive amount of data that can be transmitted to a Cloud for processing. However, one fundamental limitation of a Cloud is the connectivity with end devices. Fog computing overcomes this limitation and supports the requirements of time-sensitive applications by distributing computation, communication, and storage services along the Cloud to Things (C2T) continuum, empowering potential new applications, such as smart cities, augmented reality (AR), and virtual reality (VR). However, the adoption of Fog-based computational resources and their integration with the Cloud introduces new challenges in resource management, which requires the implementation of new strategies to guarantee compliance with the quality of service (QoS) requirements of applications. In this context, one major question is how to map the QoS requirements of applications on Fog and Cloud resources. One possible approach is to discriminate the applications arriving at the Fog into Classes of Service (CoS). This paper thus introduces a set of CoS for Fog applications which includes, the QoS requirements that best characterize these Fog applications. Moreover, this paper proposes the implementation of a typical machine learning classification methodology to discriminate Fog computing applications as a function of their QoS requirements. Furthermore, the application of this methodology is illustrated in the assessment of classifiers in terms of efficiency, accuracy, and robustness to noise. The adoption of a methodology for machine learning-based classification constitutes a first step towards the definition of QoS provisioning mechanisms in Fog computing. Moreover, classifying Fog computing applications can facilitate the decision-making process for Fog scheduler.},
author = {Guevara, Judy C. and Torres, Ricardo da S. and da Fonseca, Nelson L.S.},
doi = {10.1016/j.jnca.2020.102596},
file = {:E\:/文章/On the classification of fog computing applications A machine learning.pdf:pdf},
issn = {10958592},
journal = {Journal of Network and Computer Applications},
keywords = {Attribute noise,Classes of service,Classification algorithms,Cloud computing,Edge computing,Feature selection,Fog computing,Internet of things,Machine learning,Quality of service,Scheduling},
number = {February},
pages = {102596},
publisher = {Elsevier Ltd},
title = {{On the classification of fog computing applications: A machine learning perspective}},
url = {https://doi.org/10.1016/j.jnca.2020.102596},
volume = {159},
year = {2020}
}
@article{Tang2019,
abstract = {Fog Computing (FC) is a flexible architecture to support distributed domain-specific applications with cloud-like quality of service. However, current FC still lacks the mobility support mechanism when facing many mobile users with diversified application quality requirements. Such mobility support mechanism can be critical such as in the industrial internet where human, products, and devices are moveable. To fill in such gaps, in this paper we propose novel container migration algorithms and architecture to support mobility tasks with various application requirements. Our algorithms are realized from three aspects: 1) We consider mobile application tasks can be hosted in a container of a corresponding fog node that can be migrated, taking the communication delay and computational power consumption into consideration; 2) We further model such container migration strategy as multiple dimensional Markov Decision Process (MDP) spaces. To effectively reduce the large MDP spaces, efficient deep reinforcement learning algorithms are devised to achieve fast decision-making and 3) We implement the model and algorithms as a container migration prototype system and test its feasibility and performance. Extensive experiments show that our strategy outperforms the existing baseline approaches 2.9, 48.5 and 58.4 percent on average in terms of delay, power consumption, and migration cost, respectively.},
author = {Tang, Zhiqing and Zhou, Xiaojie and Zhang, Fuming and Jia, Weijia and Zhao, Wei},
doi = {10.1109/TSC.2018.2827070},
file = {:E\:/文章/Migration Modeling and Learning Algorithms for Containers in Fog Computing.pdf:pdf},
issn = {19391374},
journal = {IEEE Transactions on Services Computing},
keywords = {Fog computing,container migration,deep reinforcement learning,delay,power consumption,user mobility},
number = {5},
pages = {712--725},
title = {{Migration Modeling and Learning Algorithms for Containers in Fog Computing}},
volume = {12},
year = {2019}
}
@article{Eawna2015,
abstract = {Cloud computing is a model for delivering information technology services in which resources are retrieved from the internet through web-based tools and applications. The most important challenges are influenced to adopt the cloud computing technology such as security, resources allocation and resources provisioning. The only existing resource provisioning mechanisms in cloud computing using meta-heuristic technique are based on single tier application. In this paper we propose dynamic resources provisioning in multi-tier application by using meta-heuristic technique such as Particle Swarm Optimization (PSO) algorithm, Simulated Annealing (SA) algorithm and hybrid algorithm that combine Particle Swarm Optimization (PSO) and Simulated Annealing (SA). The simulation results show that resource provisioning based on PSO-SA algorithm in multi-tier application is much faster than resource provisioning in multi-tier application based on PSO algorithm and SA algorithm, that is beneficial in the development of cloud computing.},
author = {Eawna, Marwah Hashim and Mohammed, Salma Hamdy and El-Horbaty, El Sayed M.},
doi = {10.1016/j.procs.2015.09.012},
file = {:E\:/文章/Hybrid Algorithm for Resource Provisioning of Multi-tier Cloud Computing.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Cloud Computing,PSO-Particle Swarm Optimization,Resources Provisioning,SA-Simulated Annealing},
number = {Iccmit},
pages = {682--690},
publisher = {Elsevier Masson SAS},
title = {{Hybrid Algorithm for Resource Provisioning of Multi-tier Cloud Computing}},
url = {http://dx.doi.org/10.1016/j.procs.2015.09.012},
volume = {65},
year = {2015}
}
@article{Xu2019a,
abstract = {Cloud computing has been regarded as an emerging approach to provisioning resources and managing applications. It provides attractive features, such as an on-demand model, scalability enhancement, and management cost reduction. However, cloud computing systems continue to face problems such as hardware failures, overloads caused by unexpected workloads, or the waste of energy due to inefficient resource utilization, which all result in resource shortages and application issues such as delays or saturation. A paradigm, the brownout, has been applied to handle these issues by adaptively activating or deactivating optional parts of applications or services to manage resource usage in cloud computing system. Brownout has successfully shown that it can avoid overloads due to changes in workload and achieve better load balancing and energy saving effects. This article proposes a taxonomy of the brownout approach for managing resources and applications adaptively in cloud computing systems and carries out a comprehensive survey. It identifies open challenges and offers future research directions.},
author = {Xu, Minxian and Buyya, Rajkumar},
doi = {10.1145/3234151},
file = {:E\:/文章/Brownout Approach.pdf:pdf},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Adaptive management,Brownout,Cloud computing,Optional services,Quality of service},
number = {1},
title = {{Brownout approach for adaptive management of resources and applications in cloud computing systems: A taxonomy and future directions}},
volume = {52},
year = {2019}
}
@article{Ashraf2016,
abstract = {We present a prediction-based, cost-efficient Virtual Machine (VM) provisioning and admission control approach for multi-tier web applications. The proposed approach provides automatic deployment and scaling of multiple web applications on a given Infrastructure as a Service (IaaS) cloud. It monitors and uses collected resource utilization metrics itself and does not require a performance model of the applications or the infrastructure dynamics. The approach uses the OSGi component model to share VM resources among deployed applications, reducing the total number of required VMs. The proposed approach comprises three sub-approaches: a reactive VM provisioning approach called ARVUE, a hybrid reactive-proactive VM provisioning approach called Cost-efficient Resource Allocation for Multiple web applications with Proactive scaling (CRAMP), and a session-based adaptive admission control approach called adaptive Admission Control for Virtualized Application Servers (ACVAS). Performance under varying load conditions is guaranteed by automatic adjustment and tuning of the CRAMP and ACVAS parameters. The proposed approach is demonstrated in discrete-event simulations and is evaluated in a series of experiments involving synthetic as well as realistic load patterns.},
author = {Ashraf, Adnan and Byholm, Benjamin and Porres, Ivan},
doi = {10.1186/s13677-016-0065-9},
file = {:E\:/文章/Prediction-based VM provisioning and admission control for multi-tier web applications.pdf:pdf},
isbn = {1367701600},
issn = {2192113X},
journal = {Journal of Cloud Computing},
keywords = {Admission control,Cloud computing,Cost-efficiency,Performance,Virtual machine provisioning,Web application},
number = {1},
publisher = {Journal of Cloud Computing: Advances, Systems and Applications},
title = {{Prediction-based VM provisioning and admission control for multi-tier web applications}},
url = {http://dx.doi.org/10.1186/s13677-016-0065-9},
volume = {5},
year = {2016}
}
@article{Bi2015,
abstract = {Dynamic virtualised resource allocation is the key to quality of service assurance for multi-tier web application services in cloud data centre. In this paper, we develop a self-management architecture of cloud data centres with virtualisation mechanism for multi-tier web application services. Based on this architecture, we establish a flexible hybrid queueing model to determine the amount of virtual machines for each tier of virtualised application service environments. Besides, we propose a non-linear constrained optimisation problem with restrictions defined in service level agreement. Furthermore, we develop a heuristic mixed optimisation algorithm to maximise the profit of cloud infrastructure providers, and to meet performance requirements from different clients as well. Finally, we compare the effectiveness of our dynamic allocation strategy with two other allocation strategies. The simulation results show that the proposed resource allocation method is efficient in improving the overall performance and reducing the resource energy cost.},
author = {Bi, Jing and Yuan, Haitao and Tie, Ming and Tan, Wei},
doi = {10.1080/17517575.2013.830342},
file = {:E\:/文章/SLA-based optimisation of virtualised resource for multi-tier web applications in cloud data centres.pdf:pdf},
issn = {17517583},
journal = {Enterprise Information Systems},
keywords = {cloud data centre,dynamic resource allocation,multi-tier web application,performance optimisation,service level agreement (SLA),virtualisation},
number = {7},
pages = {743--767},
publisher = {Taylor & Francis},
title = {{SLA-based optimisation of virtualised resource for multi-tier web applications in cloud data centres}},
volume = {9},
year = {2015}
}
@article{Wang2019,
abstract = {Scaling complex distributed systems such as e-commerce is an importance practice to simultaneously achieve high performance and high resource efficiency in the cloud. Most previous research focuses on hardware resource scaling to handle runtime workload variation. Through extensive experiments using a representative n-tier web application benchmark (RUBBoS), we demonstrate that scaling an n-tier system by adding or removing VMs without appropriately re-allocating soft resources (e.g., server threads and connections) may lead to significant performance degradation resulting from implicit change of request processing concurrency in the system, causing either over-or under-utilization of the critical hardware resource in the system. We build a concurrency-aware model that determines a near optimal soft resource allocation of each tier by combining some operational queuing laws and the fine-grained online measurement data of the system. We then develop a dynamic concurrency management (DCM) framework that integrates the concurrency-aware model to intelligently reallocate soft resources in the system during the system scaling process. We compare DCM with Amazon EC2-AutoScale, the state-of-the-art hardware only scaling management solution using six real-world bursty workload traces. The experimental results show that DCM achieves significantly shorter tail latency and higher throughput compared to Amazon EC2-AutoScale under all the workload traces.},
author = {Wang, Qingyang and Chen, Hui and Zhang, Shungeng and Hu, Liting and Palanisamy, Balaji},
doi = {10.1109/TPDS.2018.2871086},
file = {:E\:/文章/Integrating Concurrency Control in n-Tier.pdf:pdf},
issn = {15582183},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Scalability,cloud computing,configuration,parallel processing,soft resources,web application},
number = {4},
pages = {855--869},
title = {{Integrating Concurrency Control in n-Tier Application Scaling Management in the Cloud}},
volume = {30},
year = {2019}
}
@inproceedings{Nardelli2017,
abstract = {Docker containers enable to package an application together with all its dependencies and easily run it in any environment. Thanks to their ease of use and portability, containers are gaining an increasing interest and promise to change the way how Cloud platforms are designed and managed. For their execution in the Cloud, we need to solve the container deployment problem, which deals with the identication of an elastic set of computing machines that can host and exe- cute those containers, while considering the diversity of their requirements. In this paper, we provide a general formulation of the Elastic provisioning of Virtual machines for Container Deployment (for short, EVCD) as an Integer Linear Programming problem, which takes explicitly into account the heterogeneity of container requirements and virtual machine resources. Besides optimizing multiple QoS metrics, EVCD can reallocate containers at runtime, when a QoS improvement can be achieved. Using the proposed formulation as benchmark, we evaluate two well-known heuristics, i.e., greedy First fit and round-robin, that are usually adopted for solving the container deployment problem.},
author = {Nardelli, Matteo and Hochreiner, Christoph and Schulte, Stefan},
booktitle = {ICPE 2017 - Companion of the 2017 ACM/SPEC International Conference on Performance Engineering},
doi = {10.1145/3053600.3053602},
file = {:E\:/文章/Elastic provisioning of virtual machines for container deployment.pdf:pdf},
isbn = {9781450348997},
keywords = {Cloud computing,Container,Qos,Resource allocation},
pages = {5--10},
title = {{Elastic provisioning of virtual machines for container deployment}},
year = {2017}
}
@inproceedings{Jiang2013,
abstract = {In the on-demand cloud environment, web application providers have the potential to scale virtual resources up or down to achieve cost-effective outcomes. True elasticity and cost-effectiveness in the pay-per-use cloud business model, however, have not yet been achieved. To address this challenge, we propose a novel cloud resource auto-scaling scheme at the virtual machine (VM) level for web application providers. The scheme automatically predicts the number of web requests and discovers an optimal cloud resource demand with cost-latency trade-off. Based on this demand, the scheme makes a resource scaling decision that is up or down or NOP (no operation) in each time-unit re-allocation. We have implemented the scheme on the Amazon cloud platform and evaluated it using three real-world web log datasets. Our experiment results demonstrate that the proposed scheme achieves resource auto-scaling with an optimal cost-latency trade-off, as well as low SLA violations. {\textcopyright} 2013 IEEE.},
author = {Jiang, Jing and Lu, Jie and Zhang, Guangquan and Long, Guodong},
booktitle = {Proceedings - 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2013},
doi = {10.1109/CCGrid.2013.73},
isbn = {9780768549965},
keywords = {Cloud computing,Elastic computing,Resource prediction,Resource scaling,Web services},
pages = {58--65},
publisher = {IEEE Press},
series = {CCGRID '13},
title = {{Optimal cloud resource auto-scaling for web applications}},
year = {2013}
}
@article{Diouf2020,
abstract = {Docker container virtualization technology is being widely adopted in cloud computing environments because of its lightweight and efficiency. However, it requires adequate control and management via an orchestrator. As a result, cloud providers are adopting the open-access Kubernetes platform as the standard orchestrator of containerized applications. To ensure applications' availability in Kubernetes, the latter uses Raft protocol's replication mechanism. Despite its simplicity, Raft assumes that machines fail only when shutdown. This failure event is rarely the only reason for a machine's malfunction. Indeed, software errors or malicious attacks can cause machines to exhibit Byzantine (i.e. random) behavior and thereby corrupt the accuracy and availability of the replication protocol. In this paper, we propose a Kubernetes multi-Master Robust (KmMR) platform to overcome this limitation. KmMR is based on the adaptation and integration of the BFT-SMaRt fault-tolerant replication protocol into Kubernetes environment. Unlike Raft protocol, BFT-SMaRt is resistant to both Byzantine and non-Byzantine faults. Experimental results show that KmMR is able to guarantee the continuity of services, even when the total number of tolerated faults is exceeded. In addition, KmMR provides on average a consensus time 1000 times shorter than that achieved by the conventional platform (with Raft), in such condition. Finally, we show that KmMR generates a small additional cost in terms of resource consumption compared to the conventional platform.},
archivePrefix = {arXiv},
arxivId = {1904.06206},
author = {Diouf, Gor Mack and Elbiaze, Halima and Jaafar, Wael},
doi = {10.1016/j.future.2020.03.060},
eprint = {1904.06206},
file = {:E\:/文章/On Byzantine fault tolerance in multi-master Kubernetes clusters.pdf:pdf},
issn = {0167739X},
journal = {Future Generation Computer Systems},
keywords = {Byzantine and non-Byzantine faults,Cloud computing,Docker containers,Fault tolerance,Kubernetes,Service continuity},
pages = {407--419},
publisher = {Elsevier B.V.},
title = {{On Byzantine fault tolerance in multi-master Kubernetes clusters}},
url = {https://doi.org/10.1016/j.future.2020.03.060},
volume = {109},
year = {2020}
}
@misc{Kumar2019,
author = {Kumar, Sanil and Du, Jun},
booktitle = {kubernetes.io},
title = {{KubeEdge, a Kubernetes Native Edge Computing Framework - Kubernetes}},
url = {https://kubeedge.io/en/. https://kubernetes.io/blog/2019/03/19/kubeedge-k8s-based-edge-intro/},
year = {2019}
}
@article{Bella2018,
abstract = {Recently, container-based virtualization is gaining popularity. It is a lightweight virtualization, which utilizes the capabilities of the Linux kernel to allow sandboxing processes from one another and controlling their resource allocations. One of the most used container-based virtualization today is Docker. Docker is an open source project designed for developing, shipping, and running an application inside a container-based virtualization. We can deploy several web application container using Docker to serve millions of users. It also reduces the possibility of a single point of failure in the architecture. However, managing several containers for creating a single service is a challenging task. Docker solves this problem by providing container cluster management called Docker Swarm. The Docker Swarm internal load balancing mechanism focused on how to distribute the request to the worker equally based on the user request. It does not provide any mechanism to monitor the resource utilization of each host machine. It is problematic because it can lead to unequal load distribution between the host machines. This research purpose is to distribute web server traffic inside a Docker swarm based on the resource utilization of the host machine. We focused on the memory utilization on each host machine. We propose the mechanism to monitor the memory utilization of each host machine and distribute the web traffic based on the memory utilization of each host machine. The result of our experiment is promising. Each of the worker nodes receives a fair distribution of load. This technique can decrease the possibility of a single point of failure in the web server cluster.},
author = {Bella, Mochamad Rexa Mei and Data, Mahendra and Yahya, Widhi},
doi = {10.1109/SIET.2018.8693212},
file = {:E\:/文章/Web Server Load Balancing Based On Memory Utilization Using Docker Swarm.pdf:pdf},
isbn = {9781538674079},
journal = {3rd International Conference on Sustainable Information Engineering and Technology, SIET 2018 - Proceedings},
keywords = {Docker,container,load balancing,swarm,web cluster},
pages = {220--223},
publisher = {IEEE},
title = {{Web Server Load Balancing Based On Memory Utilization Using Docker Swarm}},
year = {2018}
}
@article{Jiang2013a,
abstract = {In the on-demand cloud environment, web application providers have the potential to scale virtual resources up or down to achieve cost-effective outcomes. True elasticity and cost-effectiveness in the pay-per-use cloud business model, however, have not yet been achieved. To address this challenge, we propose a novel cloud resource auto-scaling scheme at the virtual machine (VM) level for web application providers. The scheme automatically predicts the number of web requests and discovers an optimal cloud resource demand with cost-latency trade-off. Based on this demand, the scheme makes a resource scaling decision that is up or down or NOP (no operation) in each time-unit re-allocation. We have implemented the scheme on the Amazon cloud platform and evaluated it using three real-world web log datasets. Our experiment results demonstrate that the proposed scheme achieves resource auto-scaling with an optimal cost-latency trade-off, as well as low SLA violations. {\textcopyright} 2013 IEEE.},
author = {Jiang, Jing and Lu, Jie and Zhang, Guangquan and Long, Guodong},
doi = {10.1109/CCGrid.2013.73},
file = {:E\:/文章/Optimal Cloud Resource Auto-Scaling for Web Applications.pdf:pdf},
journal = {Proceedings - 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2013},
keywords = {Cloud computing,Elastic computing,Resource prediction,Resource scaling,Web services},
number = {May 2013},
pages = {58--65},
title = {{Optimal cloud resource auto-scaling for web applications}},
year = {2013}
}
@article{Wei-guo2018,
abstract = {Currently, Google's open source container orchestration tool Kubernetes (K8s for short) has become the standard of fact for deploying containerized applications on a large scale in private, public, and hybrid cloud environments. By studying the scheduling-module of K8s source code, this paper finds that when selecting node for Pod, the module only considers the current optimal node, regardless of the use of resource costs. In order to solve this problem, this paper firstly realizes the model extraction of its scheduling module, and designs and implements the simulation experiment for the model for the first time. Secondly, a large number of papers on cloud computing resource scheduling are read. In this paper, the K8s scheduling model is improved by combining ant colony algorithm and particle swarm optimization algorithm. Finally, it is scored, and the node with the smallest objective function is selected to deploy the Pod. This paper draws on the resource scheduling model of CloudSim tool and implements resource scheduling of K8s using Java language. The experimental results show that the proposed algorithm is better than the original scheduling algorithm, which reduces the total resource cost and the maximum load of the node, and makes the task assignment more balanced.},
author = {Wei-guo, Zhang and Xi-lin, Ma and Jin-zhong, Zhang},
doi = {10.1145/3290480.3290507},
file = {:E\:/文章/Research on Kubernetes' Resource Scheduling Scheme.pdf:pdf},
isbn = {9781450365673},
journal = {ACM International Conference Proceeding Series},
keywords = {Ant colony algorithm,Cloud computing,Kubernetes,Particle swarm algorithm,Resource scheduling},
pages = {144--148},
title = {{Research on kubernetes' resource scheduling scheme}},
year = {2018}
}
@article{Patikirikorala2012,
abstract = {Control engineering approaches have been identified as a promising tool to integrate self-adaptive capabilities into software systems. Introduction of the feedback loop and controller into the management system potentially enables the software systems to achieve the runtime performance objectives and maintain the integrity of the system when they are operating in unpredictable and dynamic environments. There is a large body of literature that has proposed control engineering solutions for different application domains, handling different performance variables and control objectives. However, the relevant literature is scattered over different conference proceedings, journals and research communities. Consequently, conducting a survey to analyze and classify the existing literature is a useful, yet a challenging task. This paper presents the results of a systematic survey that includes classification and analysis of 161 papers in the existing literature. In order to capture the characteristics of the control solutions proposed in these papers we introduce a taxonomy as a basis for classification of all articles. Finally, survey results are presented, including quantitative, cross and trend analysis. {\textcopyright} 2012 IEEE.},
author = {Patikirikorala, Tharindu and Colman, Alan and Han, Jun and Wang, Liuping},
doi = {10.1109/SEAMS.2012.6224389},
file = {:E\:/文章/控制理论综述A systematic survey on the design of self-adaptive software systems using control engineering approaches.pdf:pdf},
isbn = {9781467317870},
issn = {21572305},
journal = {ICSE Workshop on Software Engineering for Adaptive and Self-Managing Systems},
pages = {33--42},
title = {{A systematic survey on the design of self-adaptive software systems using control engineering approaches}},
year = {2012}
}
@article{Zhao2016,
abstract = {Aiming at the current problems that most physical hosts in the cloud data center are so overloaded that it makes the whole cloud data center' load imbalanced and that existing load balancing approaches have relatively high complexity, this paper has focused on the selection problem of physical hosts for deploying requested tasks and proposed a novel heuristic approach called Load Balancing based on Bayes and Clustering (LB-BC). Most previous works, generally, utilize a series of algorithms through optimizing the candidate target hosts within an algorithm cycle and then picking out the optimal target hosts to achieve the immediate load balancing effect. However, the immediate effect doesn't guarantee high execution efficiency for the next task although it has abilities in achieving high resource utilization. Based on this argument, LB-BC introduces the concept of achieving the overall load balancing in a long-term process in contrast to the immediate load balancing approaches in the current literature. LB-BC makes a limited constraint about all physical hosts aiming to achieve a task deployment approach with global search capability in terms of the performance function of computing resource. The Bayes theorem is combined with the clustering process to obtain the optimal clustering set of physical hosts finally. Simulation results show that compared with the existing works, the proposed approach has reduced the failure number of task deployment events obviously, improved the throughput, and optimized the external services performance of cloud data centers.},
author = {Zhao, Jia and Yang, Kun and Wei, Xiaohui and Ding, Yan and Hu, Liang and Xu, Gaochao},
doi = {10.1109/TPDS.2015.2402655},
file = {:E\:/文章/A Heuristic Clustering-Based Task Deployment Approach for Load Balancing Using Bayes Theorem in Cloud Environment.pdf:pdf},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {Bayes Theorem,Cloud Computing,Clustering,Load Balancing,Task Deployment},
number = {2},
pages = {305--316},
title = {{A Heuristic Clustering-Based Task Deployment Approach for Load Balancing Using Bayes Theorem in Cloud Environment}},
volume = {27},
year = {2016}
}
